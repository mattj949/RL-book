{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6f059f5-a626-4467-8dd1-a921bf58dc50",
   "metadata": {},
   "source": [
    "## Template Code from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1e5c4ab-f49d-4195-a20b-6060fe6086ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence, Tuple, Mapping\n",
    "\n",
    "S = str\n",
    "DataType = Sequence[Sequence[Tuple[S, float]]]\n",
    "ProbFunc = Mapping[S, Mapping[S, float]]\n",
    "RewardFunc = Mapping[S, float]\n",
    "ValueFunc = Mapping[S, float]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c6fc753-eeaa-42a4-b2d0-99ebbd5ceae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_return_samples(\n",
    "    data: DataType\n",
    ") -> Sequence[Tuple[S, float]]:\n",
    "    \"\"\"\n",
    "    prepare sequence of (state, return) pairs.\n",
    "    Note: (state, return) pairs is not same as (state, reward) pairs.\n",
    "    \"\"\"\n",
    "    return [(s, sum(r for (_, r) in l[i:]))\n",
    "            for l in data for i, (s, _) in enumerate(l)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03732157-bd86-4be4-8707-88eadffca7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.monte_carlo import *\n",
    "from rl.markov_process import *\n",
    "from rl.function_approx import Tabular\n",
    "from itertools import islice\n",
    "\n",
    "def get_mc_value_function(\n",
    "    state_return_samples: Sequence[Tuple[S, float]]\n",
    ") -> ValueFunc:\n",
    "    \"\"\"\n",
    "    Implement tabular MC Value Function compatible with the interface defined above.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    # Once we discect the mc_prediction() function in rl.monte_carlo\n",
    "    # we see that we really only need each state and return, so creating TransitionStep objects \n",
    "    # is wholly unnecessary. Doing so would be wrapping the relevant data in an object, just to\n",
    "    # be unwrapped again. Further, the mc_prediction function wants to calculate returns from rewards,\n",
    "    # where here we only have returns directly and no access to individual rewards.\n",
    "    def helper(approx_0, episode):\n",
    "        f = approx_0\n",
    "        f = last(f.iterate_updates(\n",
    "            [(step[0], step[1])] for step in episode\n",
    "        ))\n",
    "        yield f\n",
    "        \n",
    "    num_traces = 60000\n",
    "    it = helper(approx_0 = Tabular(),\n",
    "               episode = state_return_samples)\n",
    "    \n",
    "    return last(islice(it, num_traces))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fd86810-c968-4eb4-a76a-5c76db6e494a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_reward_next_state_samples(\n",
    "    data: DataType\n",
    ") -> Sequence[Tuple[S, float, S]]:\n",
    "    \"\"\"\n",
    "    prepare sequence of (state, reward, next_state) triples.\n",
    "    \"\"\"\n",
    "    return [(s, r, l[i+1][0] if i < len(l) - 1 else 'T')\n",
    "            for l in data for i, (s, r) in enumerate(l)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "602a5456-25eb-4037-9305-831b9a1abfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.markov_process import *\n",
    "\n",
    "def get_probability_and_reward_functions(\n",
    "    srs_samples: Sequence[Tuple[S, float, S]]\n",
    ") -> Tuple[ProbFunc, RewardFunc]:\n",
    "    \"\"\"\n",
    "    Implement code that produces the probability transitions and the\n",
    "    reward function compatible with the interface defined above.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Input is (State, Reward, Next_State)\n",
    "    # ProbFunc = Mapping[S, Mapping[S, float]]\n",
    "    # RewardFunc = Mapping[S, float]\n",
    "\n",
    "    A_targets_map = {'A':0,'B':0,'T':0}\n",
    "    B_targets_map = {'A':0,'B':0,'T':0}\n",
    "    A_rewards = 0\n",
    "    num_A = 0\n",
    "    B_rewards = 0\n",
    "    num_B = 0\n",
    "    for samp in srs_samples:\n",
    "        if samp[0] == 'A':\n",
    "            A_targets_map[samp[2]] += 1\n",
    "            A_rewards += samp[1]\n",
    "            num_A += 1\n",
    "        elif samp[0] == 'B':\n",
    "            B_targets_map[samp[2]] += 1\n",
    "            B_rewards += samp[1]\n",
    "            num_B += 1\n",
    "            \n",
    "    \n",
    "    A_total_transitions = sum(A_targets_map.values())\n",
    "    for key in A_targets_map.keys():\n",
    "        A_targets_map.update({key: A_targets_map[key] / A_total_transitions})\n",
    "       \n",
    "    B_total_transitions = sum(B_targets_map.values())\n",
    "    for key in B_targets_map.keys():\n",
    "        B_targets_map.update({key: B_targets_map[key] / B_total_transitions})\n",
    "    \n",
    "    my_ProbFunc = {'A' : A_targets_map,\n",
    "                'B' : B_targets_map}\n",
    "    \n",
    "    my_RewardFunc = {'A': A_rewards / num_A,\n",
    "                  'B': B_rewards / num_B}\n",
    "    \n",
    "    \n",
    "    return (my_ProbFunc, my_RewardFunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "261f5721-ba19-4fca-a77f-e62d7e753d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mrp_value_function(\n",
    "    prob_func: ProbFunc,\n",
    "    reward_func: RewardFunc\n",
    ") -> ValueFunc:\n",
    "    \"\"\"\n",
    "    Implement code that calculates the MRP Value Function from the probability\n",
    "    transitions and reward function, compatible with the interface defined above.\n",
    "    Hint: Use the MRP Bellman Equation and simple linear algebra\n",
    "    \"\"\"\n",
    "    \n",
    "   # See pg. 132 for Bellman Equation\n",
    "    gamma = 1\n",
    "    \n",
    "    # Generate transition matrix\n",
    "    mat = np.zeros((2,2))\n",
    "    for i, s1 in enumerate(['A','B']):\n",
    "        for j, s2 in enumerate(['A','B']):\n",
    "            mat[i,j] = prob_func[s1][s2]\n",
    "    \n",
    "    # Expected reward for each state\n",
    "    reward_function_vec = np.array([reward_func['A'], reward_func['B']])\n",
    "    \n",
    "    # Direct solve for the value function vector\n",
    "    value_func_vec = np.linalg.solve(\n",
    "        np.eye(2) -\n",
    "        gamma * mat,\n",
    "        reward_function_vec\n",
    "    )\n",
    "    \n",
    "    # Turn value function vector into a dictionary\n",
    "    return {'A': value_func_vec[0], 'B': value_func_vec[1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a27fbac1-e5fd-4412-82e8-ceadac31f936",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rl.iterate as iterate\n",
    "\n",
    "def get_td_value_function(\n",
    "    srs_samples: Sequence[Tuple[S, float, S]],\n",
    "    num_updates: int = 300000,\n",
    "    learning_rate: float = 0.3,\n",
    "    learning_rate_decay: int = 30\n",
    ") -> ValueFunc:\n",
    "    \"\"\"\n",
    "    Implement tabular TD(0) (with experience replay) Value Function compatible\n",
    "    with the interface defined above. Let the step size (alpha) be:\n",
    "    learning_rate * (updates / learning_rate_decay + 1) ** -0.5\n",
    "    so that Robbins-Monro condition is satisfied for the sequence of step sizes.\n",
    "    \"\"\"\n",
    "\n",
    "    # Returns a reward of 0 if Terminal\n",
    "    def custom_extended_vf(vf, s):\n",
    "        if s not in ['A', 'B']:\n",
    "            return 0.0\n",
    "        else:\n",
    "            return vf(s)\n",
    "        \n",
    "    # Meat of the td prediction algorithm. Adapted from td_prediction \n",
    "    # in td.py\n",
    "    # The first for loop generates experience-replayed groupings\n",
    "    def custom_td(srs_samples, approx_0):\n",
    "        # Generate a new sequence with replacement\n",
    "        transitions = []\n",
    "        for i in range(len(srs_samples)):\n",
    "            rnum = np.random.randint(0,len(srs_samples))\n",
    "            transitions.append(srs_samples[rnum])\n",
    "        \n",
    "        # Apply TD updates\n",
    "        def step(v,transition):\n",
    "            return v.update([(\n",
    "                transition[0],\n",
    "                transition[1] + 1 * custom_extended_vf(v, transition[2])\n",
    "            )])\n",
    "        return iterate.accumulate(transitions, step, initial=approx_0)\n",
    "        \n",
    "        \n",
    "        \n",
    "    # Custom learning rate schedule, specified above\n",
    "    def alpha_schedule(learning_rate: float,learning_rate_decay: float\n",
    "    ) -> Callable[[int], float]:\n",
    "        def lr_func(n: int) -> float:\n",
    "            return learning_rate * (n / learning_rate_decay + 1) ** -0.5\n",
    "        return lr_func\n",
    "    \n",
    "    learning_rate_func = alpha_schedule(\n",
    "        learning_rate=learning_rate,\n",
    "        learning_rate_decay=learning_rate_decay)\n",
    "\n",
    "    # Create Tabular value approximation object\n",
    "    td_approx = Tabular(count_to_weight_func=learning_rate_func)\n",
    "    \n",
    "    # Generates an Iterator of value functions\n",
    "    td_value_functs = custom_td(srs_samples, td_approx)\n",
    "    \n",
    "    # Now we need to iterate over our iterator of value functions\n",
    "    v_num = 0\n",
    "    processed_value_funcs_td = []\n",
    "    for i in td_value_functs:\n",
    "        processed_value_funcs_td += [i]\n",
    "        v_num += 1\n",
    "        if v_num > num_updates:\n",
    "            break\n",
    "            \n",
    "    # Return the last value function (that which we have iterated/updated\n",
    "    # the most\n",
    "    return processed_value_funcs_td[-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f7e96f1c-30c0-40cf-967e-fcbf3291089e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.function_approx import LinearFunctionApprox, Weights\n",
    "\n",
    "def get_lstd_value_function(\n",
    "    srs_samples: Sequence[Tuple[S, float, S]]\n",
    ") -> ValueFunc:\n",
    "    \"\"\"\n",
    "    Implement LSTD Value Function compatible with the interface defined above.\n",
    "    Hint: Tabular is a special case of linear function approx where each feature\n",
    "    is an indicator variables for a corresponding state and each parameter is\n",
    "    the value function for the corresponding state.\n",
    "    \"\"\"\n",
    "    γ = 1\n",
    "    ε = 1e-4\n",
    "    \n",
    "    feature_functions = [lambda x: 1*(x == 'A'), lambda x: 1*(x == 'B')]\n",
    "    \n",
    "    num_features: int = len(feature_functions)\n",
    "    a_inv: np.ndarray = np.eye(num_features) / ε\n",
    "    b_vec: np.ndarray = np.zeros(num_features)\n",
    "    for samp in srs_samples:\n",
    "        phi1: np.ndarray = np.array([f(samp[0]) for f in feature_functions])\n",
    "        if isinstance(samp[2], NonTerminal):\n",
    "            phi2 = phi1 - γ * np.array([f(samp[2])\n",
    "                                        for f in feature_functions])\n",
    "        else:\n",
    "            phi2 = phi1\n",
    "        temp: np.ndarray = a_inv.T.dot(phi2)\n",
    "        a_inv = a_inv - np.outer(a_inv.dot(phi1), temp) / (1 + phi1.dot(temp))\n",
    "        b_vec += phi1 * samp[1]\n",
    "\n",
    "    opt_wts: np.ndarray = a_inv.dot(b_vec)\n",
    "    lstd_func = LinearFunctionApprox.create(\n",
    "        feature_functions=feature_functions,\n",
    "        weights=Weights.create(opt_wts)\n",
    "    )\n",
    "    \n",
    "    return lstd_func.evaluate(['A','B'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ab146f15-3f19-4b7e-bcdd-a72049df4ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A', 11.0), ('A', 9.0), ('B', 3.0), ('B', 2.0), ('A', 11.0), ('B', 8.0), ('A', 6.0), ('B', 2.0), ('B', 0.0), ('B', 11.0), ('B', 8.0), ('A', 2.0), ('B', 1.0), ('A', 15.0), ('B', 15.0), ('A', 13.0), ('B', 9.0), ('B', 5.0), ('B', 3.0), ('B', 10.0), ('B', 2.0)]\n",
      "------------- MONTE CARLO VALUE FUNCTION --------------\n",
      "Tabular(values_map={'A': 9.571428571428571, 'B': 5.642857142857142}, counts_map={'A': 7, 'B': 14}, count_to_weight_func=<function Tabular.<lambda>.<locals>.<lambda> at 0x7fb0718bca60>)\n",
      "-------------- MRP VALUE FUNCTION ----------\n",
      "{'A': 12.933333333333332, 'B': 9.6}\n",
      "------------- TD VALUE FUNCTION --------------\n",
      "Tabular(values_map={'B': 2.3780547053967886, 'A': 4.990493337255367}, counts_map={'B': 16, 'A': 5}, count_to_weight_func=<function get_td_value_function.<locals>.alpha_schedule.<locals>.lr_func at 0x7fb0718bc550>)\n",
      "------------- LSTD VALUE FUNCTION --------------\n",
      "[2.85710204 2.71426633]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    given_data: DataType = [\n",
    "        [('A', 2.), ('A', 6.), ('B', 1.), ('B', 2.)],\n",
    "        [('A', 3.), ('B', 2.), ('A', 4.), ('B', 2.), ('B', 0.)],\n",
    "        [('B', 3.), ('B', 6.), ('A', 1.), ('B', 1.)],\n",
    "        [('A', 0.), ('B', 2.), ('A', 4.), ('B', 4.), ('B', 2.), ('B', 3.)],\n",
    "        [('B', 8.), ('B', 2.)]\n",
    "    ]\n",
    "\n",
    "    sr_samps = get_state_return_samples(given_data)\n",
    "    print(sr_samps)\n",
    "\n",
    "    print(\"------------- MONTE CARLO VALUE FUNCTION --------------\")\n",
    "    print(get_mc_value_function(sr_samps))\n",
    "\n",
    "    srs_samps = get_state_reward_next_state_samples(given_data)\n",
    "    #print(srs_samps)\n",
    "    \n",
    "    pfunc, rfunc = get_probability_and_reward_functions(srs_samps)\n",
    "    print(\"-------------- MRP VALUE FUNCTION ----------\")\n",
    "    print(get_mrp_value_function(pfunc, rfunc))\n",
    "\n",
    "    print(\"------------- TD VALUE FUNCTION --------------\")\n",
    "    print(get_td_value_function(srs_samps))\n",
    "\n",
    "    print(\"------------- LSTD VALUE FUNCTION --------------\")\n",
    "    print(get_lstd_value_function(srs_samps))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a679d6-0922-455f-a2b8-c6008cc7721d",
   "metadata": {},
   "source": [
    "### They don't give the same value function. Now, for all of them A has a larger value than B, but their values are not the same. \n",
    "\n",
    "### Theoretically, the MRP value function should be exact since it does a direct liner algebra solve with the true rewards. MC doesnt use any bootstrapping, so should be unbiased. TD uses bootstrapping, so it will be biased, but it should be lower variance. LSTD is just a direct (gradient free) linear algebra solution for the batch TD solution. Again, MC and MRP should be unbiased where TD and LSTD will be biased. I wouldn't expect the bias to be quite this big, however. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
