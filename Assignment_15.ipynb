{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6f059f5-a626-4467-8dd1-a921bf58dc50",
   "metadata": {},
   "source": [
    "## Template Code from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1e5c4ab-f49d-4195-a20b-6060fe6086ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence, Tuple, Mapping\n",
    "\n",
    "S = str\n",
    "DataType = Sequence[Sequence[Tuple[S, float]]]\n",
    "ProbFunc = Mapping[S, Mapping[S, float]]\n",
    "RewardFunc = Mapping[S, float]\n",
    "ValueFunc = Mapping[S, float]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c6fc753-eeaa-42a4-b2d0-99ebbd5ceae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONE\n",
    "def get_state_return_samples(\n",
    "    data: DataType\n",
    ") -> Sequence[Tuple[S, float]]:\n",
    "    \"\"\"\n",
    "    prepare sequence of (state, return) pairs.\n",
    "    Note: (state, return) pairs is not same as (state, reward) pairs.\n",
    "    \"\"\"\n",
    "    return [(s, sum(r for (_, r) in l[i:]))\n",
    "            for l in data for i, (s, _) in enumerate(l)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03732157-bd86-4be4-8707-88eadffca7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONE:\n",
    "from rl.monte_carlo import *\n",
    "from rl.markov_process import *\n",
    "from rl.function_approx import Tabular\n",
    "from itertools import islice\n",
    "\n",
    "def get_mc_value_function(\n",
    "    state_return_samples: Sequence[Tuple[S, float]]\n",
    ") -> ValueFunc:\n",
    "    \"\"\"\n",
    "    Implement tabular MC Value Function compatible with the interface defined above.\n",
    "    \"\"\"\n",
    "    \n",
    "#     Generate list of ReturnStep objects\n",
    "#     list_RS = []\n",
    "#     for (i, tup) in enumerate(state_return_samples):\n",
    "#         if i < len(state_return_samples) - 1:\n",
    "#             current_state = tup[0]\n",
    "#             return_ = tup[1]\n",
    "#             next_state = state_return_samples[i+1][0]\n",
    "#             list_RS.append(ReturnStep(state=NonTerminal(current_state), \n",
    "#                                       next_state = next_state, \n",
    "#                                       reward = None,\n",
    "#                                       return_ = return_))\n",
    "    \n",
    "#     it: Iterator[ValueFunc] = mc_prediction(\n",
    "#         traces = [list_RS],\n",
    "#         approx_0 = Tabular(),\n",
    "#         Î³=1,\n",
    "#         episode_length_tolerance=1e-6\n",
    "#     )\n",
    "    \n",
    "#     num_traces = 60000\n",
    "    \n",
    "#     return last(islice(it, num_traces))\n",
    "\n",
    "\n",
    "    # The above is not necessary. Once we discent the mc_prediction() function in rl.monte_carlo\n",
    "    # we see that we really only need each state and return, so creating TransitionStep objects \n",
    "    # is wholly unnecessary. Doing so would be wrapping the relevant data in an object, just to\n",
    "    # be unwrapped again. Further, the mc_prediction function wants to calculate returns from rewards,\n",
    "    # where we only have returns directly and no access to individual rewards.\n",
    "    def helper(approx_0, episode):\n",
    "        f = approx_0\n",
    "        f = last(f.iterate_updates(\n",
    "            [(step[0], step[1])] for step in episode\n",
    "        ))\n",
    "        yield f\n",
    "        \n",
    "    num_traces = 60000\n",
    "    it = helper(approx_0 = Tabular(),\n",
    "               episode = state_return_samples)\n",
    "    \n",
    "    return last(islice(it, num_traces))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7fd86810-c968-4eb4-a76a-5c76db6e494a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONE\n",
    "def get_state_reward_next_state_samples(\n",
    "    data: DataType\n",
    ") -> Sequence[Tuple[S, float, S]]:\n",
    "    \"\"\"\n",
    "    prepare sequence of (state, reward, next_state) triples.\n",
    "    \"\"\"\n",
    "    return [(s, r, l[i+1][0] if i < len(l) - 1 else 'T')\n",
    "            for l in data for i, (s, r) in enumerate(l)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "602a5456-25eb-4037-9305-831b9a1abfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO:\n",
    "from rl.markov_process import *\n",
    "\n",
    "def get_probability_and_reward_functions(\n",
    "    srs_samples: Sequence[Tuple[S, float, S]]\n",
    ") -> Tuple[ProbFunc, RewardFunc]:\n",
    "    \"\"\"\n",
    "    Implement code that produces the probability transitions and the\n",
    "    reward function compatible with the interface defined above.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Input is (State, Reward, Next_State)\n",
    "    # ProbFunc = Mapping[S, Mapping[S, float]]\n",
    "    # RewardFunc = Mapping[S, float]\n",
    "\n",
    "    A_targets_map = {'A':0,'B':0,'T':0}\n",
    "    B_targets_map = {'A':0,'B':0,'T':0}\n",
    "    A_rewards = 0\n",
    "    num_A = 0\n",
    "    B_rewards = 0\n",
    "    num_B = 0\n",
    "    for samp in srs_samples:\n",
    "        if samp[0] == 'A':\n",
    "            A_targets_map[samp[2]] += 1\n",
    "            A_rewards += samp[1]\n",
    "            num_A += 1\n",
    "        elif samp[0] == 'B':\n",
    "            B_targets_map[samp[2]] += 1\n",
    "            B_rewards += samp[1]\n",
    "            num_B += 1\n",
    "            \n",
    "    \n",
    "    A_total_transitions = sum(A_targets_map.values())\n",
    "    for key in A_targets_map.keys():\n",
    "        A_targets_map.update({key: A_targets_map[key] / A_total_transitions})\n",
    "       \n",
    "    B_total_transitions = sum(B_targets_map.values())\n",
    "    for key in B_targets_map.keys():\n",
    "        B_targets_map.update({key: B_targets_map[key] / B_total_transitions})\n",
    "    \n",
    "    my_ProbFunc = {'A' : A_targets_map,\n",
    "                'B' : B_targets_map}\n",
    "    \n",
    "    my_RewardFunc = {'A': A_rewards / num_A,\n",
    "                  'B': B_rewards / num_B}\n",
    "    \n",
    "    \n",
    "    return (my_ProbFunc, my_RewardFunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "261f5721-ba19-4fca-a77f-e62d7e753d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONE\n",
    "from rl.dynamic_programming import *\n",
    "def get_mrp_value_function(\n",
    "    prob_func: ProbFunc,\n",
    "    reward_func: RewardFunc\n",
    ") -> ValueFunc:\n",
    "    \"\"\"\n",
    "    Implement code that calculates the MRP Value Function from the probability\n",
    "    transitions and reward function, compatible with the interface defined above.\n",
    "    Hint: Use the MRP Bellman Equation and simple linear algebra\n",
    "    \"\"\"\n",
    "    \n",
    "   # See pg. 132 \n",
    "    gamma = 1\n",
    "    \n",
    "    # Generate transition matrix\n",
    "    mat = np.zeros((2,2))\n",
    "    for i, s1 in enumerate(['A','B']):\n",
    "        for j, s2 in enumerate(['A','B']):\n",
    "            mat[i,j] = prob_func[s1][s2]\n",
    "    \n",
    "    # Expected reward for each state\n",
    "    reward_function_vec = np.array([reward_func['A'], reward_func['B']])\n",
    "    \n",
    "    # Direct solve for the value function vector\n",
    "    value_func_vec = np.linalg.solve(\n",
    "        np.eye(2) -\n",
    "        gamma * mat,\n",
    "        reward_function_vec\n",
    "    )\n",
    "    \n",
    "    # Turn value function vector into a dictionary\n",
    "    return {'A': value_func_vec[0], 'B': value_func_vec[1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27fbac1-e5fd-4412-82e8-ceadac31f936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO:\n",
    "def get_td_value_function(\n",
    "    srs_samples: Sequence[Tuple[S, float, S]],\n",
    "    num_updates: int = 300000,\n",
    "    learning_rate: float = 0.3,\n",
    "    learning_rate_decay: int = 30\n",
    ") -> ValueFunc:\n",
    "    \"\"\"\n",
    "    Implement tabular TD(0) (with experience replay) Value Function compatible\n",
    "    with the interface defined above. Let the step size (alpha) be:\n",
    "    learning_rate * (updates / learning_rate_decay + 1) ** -0.5\n",
    "    so that Robbins-Monro condition is satisfied for the sequence of step sizes.\n",
    "    \"\"\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e96f1c-30c0-40cf-967e-fcbf3291089e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO:\n",
    "def get_lstd_value_function(\n",
    "    srs_samples: Sequence[Tuple[S, float, S]]\n",
    ") -> ValueFunc:\n",
    "    \"\"\"\n",
    "    Implement LSTD Value Function compatible with the interface defined above.\n",
    "    Hint: Tabular is a special case of linear function approx where each feature\n",
    "    is an indicator variables for a corresponding state and each parameter is\n",
    "    the value function for the corresponding state.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ab146f15-3f19-4b7e-bcdd-a72049df4ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A', 11.0), ('A', 9.0), ('B', 3.0), ('B', 2.0), ('A', 11.0), ('B', 8.0), ('A', 6.0), ('B', 2.0), ('B', 0.0), ('B', 11.0), ('B', 8.0), ('A', 2.0), ('B', 1.0), ('A', 15.0), ('B', 15.0), ('A', 13.0), ('B', 9.0), ('B', 5.0), ('B', 3.0), ('B', 10.0), ('B', 2.0)]\n",
      "------------- MONTE CARLO VALUE FUNCTION --------------\n",
      "Tabular(values_map={'A': 9.571428571428571, 'B': 5.642857142857142}, counts_map={'A': 7, 'B': 14}, count_to_weight_func=<function Tabular.<lambda>.<locals>.<lambda> at 0x7fa6080b41f0>)\n",
      "[('A', 2.0, 'A'), ('A', 6.0, 'B'), ('B', 1.0, 'B'), ('B', 2.0, 'T'), ('A', 3.0, 'B'), ('B', 2.0, 'A'), ('A', 4.0, 'B'), ('B', 2.0, 'B'), ('B', 0.0, 'T'), ('B', 3.0, 'B'), ('B', 6.0, 'A'), ('A', 1.0, 'B'), ('B', 1.0, 'T'), ('A', 0.0, 'B'), ('B', 2.0, 'A'), ('A', 4.0, 'B'), ('B', 4.0, 'B'), ('B', 2.0, 'B'), ('B', 3.0, 'T'), ('B', 8.0, 'B'), ('B', 2.0, 'T')]\n",
      "-------------- MRP VALUE FUNCTION ----------\n",
      "{'A': 12.933333333333332, 'B': 9.6}\n",
      "------------- TD VALUE FUNCTION --------------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'get_td_value_function' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fj/qb_tch8j6sdg7th8gw7q9w000000gn/T/ipykernel_15977/8264102.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"------------- TD VALUE FUNCTION --------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_td_value_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrs_samps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"------------- LSTD VALUE FUNCTION --------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_td_value_function' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    given_data: DataType = [\n",
    "        [('A', 2.), ('A', 6.), ('B', 1.), ('B', 2.)],\n",
    "        [('A', 3.), ('B', 2.), ('A', 4.), ('B', 2.), ('B', 0.)],\n",
    "        [('B', 3.), ('B', 6.), ('A', 1.), ('B', 1.)],\n",
    "        [('A', 0.), ('B', 2.), ('A', 4.), ('B', 4.), ('B', 2.), ('B', 3.)],\n",
    "        [('B', 8.), ('B', 2.)]\n",
    "    ]\n",
    "\n",
    "    sr_samps = get_state_return_samples(given_data)\n",
    "    print(sr_samps)\n",
    "\n",
    "    print(\"------------- MONTE CARLO VALUE FUNCTION --------------\")\n",
    "    print(get_mc_value_function(sr_samps))\n",
    "\n",
    "    srs_samps = get_state_reward_next_state_samples(given_data)\n",
    "    print(srs_samps)\n",
    "    \n",
    "    pfunc, rfunc = get_probability_and_reward_functions(srs_samps)\n",
    "    print(\"-------------- MRP VALUE FUNCTION ----------\")\n",
    "    print(get_mrp_value_function(pfunc, rfunc))\n",
    "\n",
    "    print(\"------------- TD VALUE FUNCTION --------------\")\n",
    "    print(get_td_value_function(srs_samps))\n",
    "\n",
    "    print(\"------------- LSTD VALUE FUNCTION --------------\")\n",
    "    print(get_lstd_value_function(srs_samps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5983dc-2e2a-407a-aee4-68136c047a7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
