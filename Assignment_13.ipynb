{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d02eb1c-ee16-4884-af91-d85183855eda",
   "metadata": {},
   "source": [
    "GLIE: Greedy in the Limit with Infinite Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e5810a4-4285-4b84-8a15-6b9e4c4acf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.monte_carlo import *\n",
    "\n",
    "from rl.function_approx import Tabular\n",
    "from rl.distribution import Choose\n",
    "from rl.chapter3.simple_inventory_mdp_cap import InventoryState\n",
    "from rl.chapter10.prediction_utils import *\n",
    "\n",
    "from rl.chapter3.simple_inventory_mdp_cap import SimpleInventoryMDPCap\n",
    "from rl.dynamic_programming import value_iteration_result\n",
    "\n",
    "from rl.distribution import Constant\n",
    "from rl.dynamic_programming import V\n",
    "import itertools\n",
    "import rl.iterate as iterate\n",
    "from rl.markov_decision_process import FiniteMarkovDecisionProcess\n",
    "from rl.policy import FiniteDeterministicPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6bdbe16-179f-449c-9337-d26e675bd69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "capacity: int = 2\n",
    "poisson_lambda: float = 1.0\n",
    "holding_cost: float = 1.0\n",
    "stockout_cost: float = 10.0\n",
    "gamma: float = 0.9\n",
    "si_mdp: SimpleInventoryMDPCap = SimpleInventoryMDPCap(\n",
    "    capacity=capacity,\n",
    "    poisson_lambda=poisson_lambda,\n",
    "    holding_cost=holding_cost,\n",
    "    stockout_cost=stockout_cost\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f03f825c-571e-47f2-bd9a-e007e6f236a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Optimal Value Function\n",
      "{NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -34.894855194671294,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -27.66095964467877,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -27.99189950444479,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -28.66095964467877,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -28.99189950444479,\n",
      " NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -29.991899504444792}\n",
      "True Optimal Policy\n",
      "For State InventoryState(on_hand=0, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=1): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=2): Do Action 0\n",
      "For State InventoryState(on_hand=1, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=1, on_order=1): Do Action 0\n",
      "For State InventoryState(on_hand=2, on_order=0): Do Action 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "true_opt_vf, true_opt_policy = value_iteration_result(si_mdp, gamma=gamma)\n",
    "print(\"True Optimal Value Function\")\n",
    "pprint(true_opt_vf)\n",
    "print(\"True Optimal Policy\")\n",
    "print(true_opt_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cc923f7-6b78-45cc-abaf-e2b652abf826",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_length_tolerance: float = 1e-5\n",
    "epsilon_as_func_of_episodes: Callable[[int], float] = lambda k: k ** -0.5\n",
    "\n",
    "initial_learning_rate: float = 0.1\n",
    "half_life: float = 10000.0\n",
    "exponent: float = 1.0\n",
    "\n",
    "# Uniform sampling across state space:\n",
    "initial_qvf_dict = {\n",
    "    (s, a): 0. for s in si_mdp.non_terminal_states for a in si_mdp.actions(s)\n",
    "}\n",
    "learning_rate_func: Callable[[int], float] = learning_rate_schedule(\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    half_life=half_life,\n",
    "    exponent=exponent\n",
    ")\n",
    "qvfs = glie_mc_control(\n",
    "    mdp=si_mdp,\n",
    "    states=Choose(si_mdp.non_terminal_states),\n",
    "    approx_0=Tabular(\n",
    "        values_map=initial_qvf_dict,\n",
    "        count_to_weight_func=learning_rate_func\n",
    "    ),\n",
    "    γ=gamma,\n",
    "    ϵ_as_func_of_episodes=epsilon_as_func_of_episodes,\n",
    "    episode_length_tolerance=episode_length_tolerance\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f97081bf-211c-4069-8d90-0fc93293059d",
   "metadata": {},
   "outputs": [],
   "source": [
    "QTabular = Tabular[Tuple[NonTerminal[S],A]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d30c682d-ed4b-4e1b-b36d-b25356524f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLIE MC Optimal Value Function with {num_episodes:d} episodes\n",
      "{NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -34.96476984562754,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -27.53625227302578,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -28.586970994259534,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -28.581369243292652,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -28.825334895696155,\n",
      " NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -30.1696604747163}\n",
      "GLIE MC Optimal Policy with {num_episodes:d} episodes\n",
      "For State InventoryState(on_hand=0, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=1): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=2): Do Action 0\n",
      "For State InventoryState(on_hand=1, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=1, on_order=1): Do Action 0\n",
      "For State InventoryState(on_hand=2, on_order=0): Do Action 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tabular Monte-Carlo Control\n",
    "\n",
    "num_episodes = 10000\n",
    "final_qvf: QTabular[InventoryState, int] = \\\n",
    "    iterate.last(itertools.islice(qvfs, num_episodes))\n",
    "\n",
    "def tabular_get_vf_and_policy_from_qvf(\n",
    "    mdp: FiniteMarkovDecisionProcess[S, A],\n",
    "    qvf: QTabular[S, A]\n",
    ") -> Tuple[V[S], FiniteDeterministicPolicy[S, A]]:\n",
    "    opt_vf: V[S] = {\n",
    "        s: max(qvf((s, a)) for a in mdp.actions(s))\n",
    "        for s in mdp.non_terminal_states\n",
    "    }\n",
    "    opt_policy: FiniteDeterministicPolicy[S, A] = \\\n",
    "        FiniteDeterministicPolicy({\n",
    "            s.state: qvf.argmax((s, a) for a in mdp.actions(s))[1]\n",
    "            for s in mdp.non_terminal_states\n",
    "        })\n",
    "    return opt_vf, opt_policy\n",
    "opt_vf, opt_policy = tabular_get_vf_and_policy_from_qvf(\n",
    "    mdp=si_mdp,\n",
    "    qvf=final_qvf\n",
    ")\n",
    "print(\"GLIE MC Optimal Value Function with {num_episodes:d} episodes\")\n",
    "pprint(opt_vf)\n",
    "print(\"GLIE MC Optimal Policy with {num_episodes:d} episodes\")\n",
    "print(opt_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a12ff9-6409-4e75-be2c-350672b472c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte-Carlo Control with Generic Function Approximation\n",
    "\n",
    "num_episodes = 10000\n",
    "final_qvf: QValueFunctionApprox[InventoryState, int] = \\\n",
    "    iterate.last(itertools.islice(qvfs, num_episodes))\n",
    "\n",
    "def get_vf_and_policy_from_qvf(\n",
    "    mdp: FiniteMarkovDecisionProcess[S, A],\n",
    "    qvf: QValueFunctionApprox[S, A]\n",
    ") -> Tuple[V[S], FiniteDeterministicPolicy[S, A]]:\n",
    "    opt_vf: V[S] = {\n",
    "        s: max(qvf((s, a)) for a in mdp.actions(s))\n",
    "        for s in mdp.non_terminal_states\n",
    "    }\n",
    "    opt_policy: FiniteDeterministicPolicy[S, A] = \\\n",
    "        FiniteDeterministicPolicy({\n",
    "            s.state: qvf.argmax((s, a) for a in mdp.actions(s))[1]\n",
    "            for s in mdp.non_terminal_states\n",
    "        })\n",
    "    return opt_vf, opt_policy\n",
    "opt_vf, opt_policy = get_vf_and_policy_from_qvf(\n",
    "    mdp=si_mdp,\n",
    "    qvf=final_qvf\n",
    ")\n",
    "print(\"GLIE MC Optimal Value Function with {num_episodes:d} episodes\")\n",
    "pprint(opt_vf)\n",
    "print(\"GLIE MC Optimal Policy with {num_episodes:d} episodes\")\n",
    "print(opt_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e702903f-133d-4b41-8551-39d14b3327e9",
   "metadata": {},
   "source": [
    "### Copy the above as the generic \"Function Approximation\" case. Replace this version with the Tabular setting. Might need to do a reclassification of Tabuluar as was done for QValueFunctionApprox[S,A]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816069d7-6a17-4981-a3aa-b87b9cd7f3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implement Asset_alloc_discrete test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
