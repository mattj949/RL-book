{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d02eb1c-ee16-4884-af91-d85183855eda",
   "metadata": {},
   "source": [
    "GLIE: Greedy in the Limit with Infinite Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e5810a4-4285-4b84-8a15-6b9e4c4acf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.monte_carlo import *\n",
    "\n",
    "from rl.function_approx import Tabular\n",
    "from rl.distribution import Choose\n",
    "from rl.chapter3.simple_inventory_mdp_cap import InventoryState\n",
    "from rl.chapter10.prediction_utils import *\n",
    "\n",
    "from rl.chapter3.simple_inventory_mdp_cap import SimpleInventoryMDPCap\n",
    "from rl.dynamic_programming import value_iteration_result\n",
    "\n",
    "from rl.distribution import Constant\n",
    "from rl.dynamic_programming import V\n",
    "import itertools\n",
    "import rl.iterate as iterate\n",
    "from rl.markov_decision_process import FiniteMarkovDecisionProcess\n",
    "from rl.policy import FiniteDeterministicPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6bdbe16-179f-449c-9337-d26e675bd69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Inventory MDP\n",
    "capacity: int = 2\n",
    "poisson_lambda: float = 1.0\n",
    "holding_cost: float = 1.0\n",
    "stockout_cost: float = 10.0\n",
    "gamma: float = 0.9\n",
    "si_mdp: SimpleInventoryMDPCap = SimpleInventoryMDPCap(\n",
    "    capacity=capacity,\n",
    "    poisson_lambda=poisson_lambda,\n",
    "    holding_cost=holding_cost,\n",
    "    stockout_cost=stockout_cost\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f03f825c-571e-47f2-bd9a-e007e6f236a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Optimal Value Function\n",
      "{NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -27.66095964467877,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -27.99189950444479,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -28.66095964467877,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -28.99189950444479,\n",
      " NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -29.991899504444792,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -34.894855194671294}\n",
      "True Optimal Policy\n",
      "For State InventoryState(on_hand=0, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=1): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=2): Do Action 0\n",
      "For State InventoryState(on_hand=1, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=1, on_order=1): Do Action 0\n",
      "For State InventoryState(on_hand=2, on_order=0): Do Action 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get Value Function and Optimal Policy via Dynamic Programming\n",
    "true_opt_vf, true_opt_policy = value_iteration_result(si_mdp, gamma=gamma)\n",
    "print(\"True Optimal Value Function\")\n",
    "pprint(true_opt_vf)\n",
    "print(\"True Optimal Policy\")\n",
    "print(true_opt_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ba89c8-89f8-4c77-a0e1-5ec693eb538b",
   "metadata": {},
   "source": [
    "## Implementing MC Control with GLIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cc923f7-6b78-45cc-abaf-e2b652abf826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definitions\n",
    "\n",
    "episode_length_tolerance: float = 1e-5\n",
    "# GLIE:\n",
    "epsilon_as_func_of_episodes: Callable[[int], float] = lambda k: k ** -1\n",
    "\n",
    "initial_learning_rate: float = 0.1\n",
    "half_life: float = 10000.0\n",
    "exponent: float = 1.0\n",
    "\n",
    "# Uniform sampling across state space:\n",
    "initial_qvf_dict = {\n",
    "    (s, a): 0. for s in si_mdp.non_terminal_states for a in si_mdp.actions(s)\n",
    "}\n",
    "\n",
    "# Function to control learning rate\n",
    "learning_rate_func: Callable[[int], float] = learning_rate_schedule(\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    half_life=half_life,\n",
    "    exponent=exponent\n",
    ")\n",
    "\n",
    "# Redefine Tabular for QValue Function Approximation\n",
    "QTabular = Tabular[Tuple[NonTerminal[S],A]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e233f581-4e3d-4d7d-bb83-0fda3b30e5c8",
   "metadata": {},
   "source": [
    "### Tabular MC Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06a95411-a547-4e8b-98a8-36a8b2321445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabular Monte Carlo Control:\n",
    "def tabular_glie_mc_control(\n",
    "    mdp: MarkovDecisionProcess[S, A],\n",
    "    states: NTStateDistribution[S],\n",
    "    approx_0: QTabular[S, A],\n",
    "    γ: float,\n",
    "    ϵ_as_func_of_episodes: Callable[[int], float],\n",
    "    episode_length_tolerance: float = 1e-6\n",
    ") -> Iterator[QTabular[S, A]]:\n",
    "    \n",
    "    q: QTabular[S, A] = approx_0\n",
    "    p: Policy[S, A] = epsilon_greedy_policy(q, mdp, 1.0) # Start off with epsilon = 1/1 = 1\n",
    "    yield q\n",
    "\n",
    "    num_episodes: int = 0\n",
    "    while True:\n",
    "        trace: Iterable[TransitionStep[S, A]] = \\\n",
    "            mdp.simulate_actions(states, p)\n",
    "        num_episodes += 1\n",
    "        for step in returns(trace, γ, episode_length_tolerance):\n",
    "            q = q.update([((step.state, step.action), step.return_)])\n",
    "        p = epsilon_greedy_policy(q, mdp, ϵ_as_func_of_episodes(num_episodes))\n",
    "        yield q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16188604-f019-46af-a6d5-63f79fd120ca",
   "metadata": {},
   "source": [
    "### Test Tabular MC Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10d4a17f-8e50-4de0-b94b-df3f526ef256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLIE MC Optimal Value Function with {num_episodes:d} episodes\n",
      "{NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -27.737845976848455,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -28.890752606818577,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -28.94435700606091,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -29.273299704154397,\n",
      " NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -30.745836703595206,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -36.00349163670859}\n",
      "GLIE MC Optimal Policy with {num_episodes:d} episodes\n",
      "For State InventoryState(on_hand=0, on_order=0): Do Action 2\n",
      "For State InventoryState(on_hand=0, on_order=1): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=2): Do Action 0\n",
      "For State InventoryState(on_hand=1, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=1, on_order=1): Do Action 0\n",
      "For State InventoryState(on_hand=2, on_order=0): Do Action 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test Tabular MC Control:\n",
    "\n",
    "from rl.chapter11.control_utils import get_vf_and_policy_from_qvf\n",
    "\n",
    "qvfs = tabular_glie_mc_control(\n",
    "    mdp=si_mdp,\n",
    "    states=Choose(si_mdp.non_terminal_states),\n",
    "    approx_0=QTabular(\n",
    "        values_map=initial_qvf_dict,\n",
    "        count_to_weight_func=learning_rate_func\n",
    "    ),\n",
    "    γ=gamma,\n",
    "    ϵ_as_func_of_episodes=epsilon_as_func_of_episodes,\n",
    "    episode_length_tolerance=episode_length_tolerance\n",
    ")\n",
    "\n",
    "num_episodes = 10000\n",
    "final_qvf: QTabular[InventoryState, int] = \\\n",
    "    iterate.last(itertools.islice(qvfs, num_episodes))\n",
    "\n",
    "# def tabular_get_vf_and_policy_from_qvf(\n",
    "#     mdp: FiniteMarkovDecisionProcess[S, A],\n",
    "#     qvf: QTabular[S, A]\n",
    "# ) -> Tuple[V[S], FiniteDeterministicPolicy[S, A]]:\n",
    "#     opt_vf: V[S] = {\n",
    "#         s: max(qvf((s, a)) for a in mdp.actions(s))\n",
    "#         for s in mdp.non_terminal_states\n",
    "#     }\n",
    "#     opt_policy: FiniteDeterministicPolicy[S, A] = \\\n",
    "#         FiniteDeterministicPolicy({\n",
    "#             s.state: qvf.argmax((s, a) for a in mdp.actions(s))[1]\n",
    "#             for s in mdp.non_terminal_states\n",
    "#         })\n",
    "#     return opt_vf, opt_policy\n",
    "opt_vf, opt_policy = get_vf_and_policy_from_qvf(\n",
    "    mdp=si_mdp,\n",
    "    qvf=final_qvf\n",
    ")\n",
    "print(\"GLIE MC Optimal Value Function with {num_episodes:d} episodes\")\n",
    "pprint(opt_vf)\n",
    "print(\"GLIE MC Optimal Policy with {num_episodes:d} episodes\")\n",
    "print(opt_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557ded26-05b3-4956-8fbd-0c8bb3ac98ab",
   "metadata": {},
   "source": [
    "### General MC Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfe24137-8916-4a5a-8c63-a78d45d8602e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Monte Carlo Control:\n",
    "def glie_mc_control(\n",
    "    mdp: MarkovDecisionProcess[S, A],\n",
    "    states: NTStateDistribution[S],\n",
    "    approx_0: QValueFunctionApprox[S, A],\n",
    "    γ: float,\n",
    "    ϵ_as_func_of_episodes: Callable[[int], float],\n",
    "    episode_length_tolerance: float = 1e-6\n",
    ") -> Iterator[QValueFunctionApprox[S, A]]:\n",
    "   \n",
    "    q: QValueFunctionApprox[S, A] = approx_0\n",
    "    p: Policy[S, A] = epsilon_greedy_policy(q, mdp, 1.0) # Start off with epsilon = 1/1 = 1\n",
    "    yield q\n",
    "\n",
    "    num_episodes: int = 0\n",
    "    while True:\n",
    "        trace: Iterable[TransitionStep[S, A]] = \\\n",
    "            mdp.simulate_actions(states, p)\n",
    "        num_episodes += 1\n",
    "        for step in returns(trace, γ, episode_length_tolerance):\n",
    "            q = q.update([((step.state, step.action), step.return_)])\n",
    "        p = epsilon_greedy_policy(q, mdp, ϵ_as_func_of_episodes(num_episodes))\n",
    "        yield q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b1131c-580c-4b3f-817c-2f483f5c58f6",
   "metadata": {},
   "source": [
    "## Implement Asset_alloc_discrete test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af55257a-2d33-4ff2-9c70-4ab3a644daee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.chapter7.asset_alloc_discrete import *\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98964593-3124-439f-9378-5c11c2e9b0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps: int = 4\n",
    "μ: float = 0.13\n",
    "σ: float = 0.2\n",
    "r: float = 0.07\n",
    "a: float = 1.0\n",
    "init_wealth: float = 1.0\n",
    "init_wealth_stdev: float = 0.1\n",
    "\n",
    "excess: float = μ - r\n",
    "var: float = σ * σ\n",
    "base_alloc: float = excess / (a * var)\n",
    "\n",
    "risky_ret: Sequence[Gaussian] = [Gaussian(μ=μ, σ=σ) for _ in range(steps)]\n",
    "riskless_ret: Sequence[float] = [r for _ in range(steps)]\n",
    "utility_function: Callable[[float], float] = lambda x: - np.exp(-a * x) / a\n",
    "alloc_choices: Sequence[float] = np.linspace(\n",
    "    2 / 3 * base_alloc,\n",
    "    4 / 3 * base_alloc,\n",
    "    11\n",
    ")\n",
    "feature_funcs: Sequence[Callable[[Tuple[float, float]], float]] = \\\n",
    "    [\n",
    "        lambda _: 1.,\n",
    "        lambda w_x: w_x[0],\n",
    "        lambda w_x: w_x[1],\n",
    "        lambda w_x: w_x[1] * w_x[1]\n",
    "    ]\n",
    "dnn: DNNSpec = DNNSpec(\n",
    "    neurons=[],\n",
    "    bias=False,\n",
    "    hidden_activation=lambda x: x,\n",
    "    hidden_activation_deriv=lambda y: np.ones_like(y),\n",
    "    output_activation=lambda x: - np.sign(a) * np.exp(-x),\n",
    "    output_activation_deriv=lambda y: -y\n",
    ")\n",
    "init_wealth_distr: Gaussian = Gaussian(μ=init_wealth, σ=init_wealth_stdev)\n",
    "\n",
    "aad: AssetAllocDiscrete = AssetAllocDiscrete(\n",
    "    risky_return_distributions=risky_ret,\n",
    "    riskless_returns=riskless_ret,\n",
    "    utility_func=utility_function,\n",
    "    risky_alloc_choices=alloc_choices,\n",
    "    feature_functions=feature_funcs,\n",
    "    dnn_spec=dnn,\n",
    "    initial_wealth_distribution=init_wealth_distr\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62ce5d35-f446-4748-b843-3981f9bd7138",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa_mdp = aad.get_mdp(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4dce5dbe-7335-46f9-be9b-77a6c2fb9832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backward Induction: VF And Policy\n",
      "---------------------------------\n",
      "\n",
      "Time 0\n",
      "\n",
      "Opt Risky Allocation = 1.00, Opt Val = -0.212\n",
      "Weights\n",
      "[[0.23875784 1.31068248]]\n",
      "\n",
      "Time 1\n",
      "\n",
      "Opt Risky Allocation = 1.50, Opt Val = -0.246\n",
      "Weights\n",
      "[[0.17572502 1.22807366]]\n",
      "\n",
      "Time 2\n",
      "\n",
      "Opt Risky Allocation = 1.40, Opt Val = -0.283\n",
      "Weights\n",
      "[[0.11566499 1.14756727]]\n",
      "\n",
      "Time 3\n",
      "\n",
      "Opt Risky Allocation = 1.20, Opt Val = -0.323\n",
      "Weights\n",
      "[[0.05855587 1.07062364]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Optimal Value Function via ADP\n",
    "\n",
    "vf_ff: Sequence[Callable[[NonTerminal[float]], float]] = [lambda _: 1., lambda w: w.state]\n",
    "it_vf: Iterator[Tuple[DNNApprox[NonTerminal[float]], DeterministicPolicy[float, float]]] = \\\n",
    "    aad.backward_induction_vf_and_pi(vf_ff)\n",
    "\n",
    "print(\"Backward Induction: VF And Policy\")\n",
    "print(\"---------------------------------\")\n",
    "print()\n",
    "for t, (v, p) in enumerate(it_vf):\n",
    "    print(f\"Time {t:d}\")\n",
    "    print()\n",
    "    opt_alloc: float = p.action_for(init_wealth)\n",
    "    val: float = v(NonTerminal(init_wealth))\n",
    "    print(f\"Opt Risky Allocation = {opt_alloc:.2f}, Opt Val = {val:.3f}\")\n",
    "    print(\"Weights\")\n",
    "    for w in v.weights:\n",
    "        print(w.weights)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5ca0cd-86dc-4d61-a9c6-822f1e4abdc7",
   "metadata": {},
   "source": [
    "## Test General MC Control on AssetAllocDiscrete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46aa1ec2-1efd-4bcf-8611-d3050d7ad2d3",
   "metadata": {},
   "source": [
    "## ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "95906069-6b54-4737-90a8-8e8bb46be21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fj/qb_tch8j6sdg7th8gw7q9w000000gn/T/ipykernel_15074/3507441780.py:33: RuntimeWarning: overflow encountered in exp\n",
      "  output_activation=lambda x: - np.sign(a) * np.exp(-x),\n"
     ]
    }
   ],
   "source": [
    "qvfs = glie_mc_control(\n",
    "    mdp = aa_mdp,\n",
    "    states = aad.get_states_distribution(1), #<--- What to do here???\n",
    "    approx_0 = aad.get_qvf_func_approx(),\n",
    "    γ=gamma,\n",
    "    ϵ_as_func_of_episodes=epsilon_as_func_of_episodes,\n",
    "    episode_length_tolerance=episode_length_tolerance\n",
    ")\n",
    "\n",
    "\n",
    "final_qvf: QValueFunctionApprox[InventoryState, int] = \\\n",
    "    iterate.last(itertools.islice(qvfs, num_episodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1056f4ac-2099-4e08-a2d6-bb55dbc26ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## How to do this?\n",
    "\n",
    "nonterminal_states = [NonTerminal(i) for i in np.linspace(0,1,10)]\n",
    "\n",
    "opt_vf: V[S] = {\n",
    "        s: max(final_qvf((s, a)) for a in aa_mdp.actions(s))\n",
    "        for s in nonterminal_states\n",
    "}\n",
    "opt_policy: FiniteDeterministicPolicy[S, A] = \\\n",
    "    FiniteDeterministicPolicy({\n",
    "        s.state: final_qvf.argmax((s, a) for a in aa_mdp.actions(s))[1]\n",
    "        for s in nonterminal_states\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4eae36f9-5e9e-4044-b43d-54be47923adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLIE MC Optimal Value Function\n",
      "{NonTerminal(state=0.5555555555555556): -2.6798010817957413e-18,\n",
      " NonTerminal(state=0.4444444444444444): -2.685339600919735e-18,\n",
      " NonTerminal(state=0.2222222222222222): -2.696451003278292e-18,\n",
      " NonTerminal(state=1.0): -2.6577610020234417e-18,\n",
      " NonTerminal(state=0.6666666666666666): -2.674273985880222e-18,\n",
      " NonTerminal(state=0.0): -2.7076083824147325e-18,\n",
      " NonTerminal(state=0.8888888888888888): -2.6632539694816263e-18,\n",
      " NonTerminal(state=0.7777777777777777): -2.668758289612784e-18,\n",
      " NonTerminal(state=0.3333333333333333): -2.6908895668612913e-18,\n",
      " NonTerminal(state=0.1111111111111111): -2.7020239338775146e-18}\n",
      "GLIE MC Optimal Policy\n",
      "For State 0.0: Do Action 1.9999999999999993\n",
      "For State 0.1111111111111111: Do Action 1.9999999999999993\n",
      "For State 0.2222222222222222: Do Action 1.9999999999999993\n",
      "For State 0.3333333333333333: Do Action 1.9999999999999993\n",
      "For State 0.4444444444444444: Do Action 1.9999999999999993\n",
      "For State 0.5555555555555556: Do Action 1.9999999999999993\n",
      "For State 0.6666666666666666: Do Action 1.9999999999999993\n",
      "For State 0.7777777777777777: Do Action 1.9999999999999993\n",
      "For State 0.8888888888888888: Do Action 1.9999999999999993\n",
      "For State 1.0: Do Action 1.9999999999999993\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"GLIE MC Optimal Value Function\")\n",
    "pprint(opt_vf)\n",
    "print(\"GLIE MC Optimal Policy\")\n",
    "print(opt_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d0cadb-ea0c-4db9-bc08-cb99e563b609",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
