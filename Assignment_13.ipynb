{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d02eb1c-ee16-4884-af91-d85183855eda",
   "metadata": {},
   "source": [
    "GLIE: Greedy in the Limit with Infinite Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e5810a4-4285-4b84-8a15-6b9e4c4acf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.monte_carlo import *\n",
    "\n",
    "from rl.function_approx import Tabular\n",
    "from rl.distribution import Choose\n",
    "from rl.chapter3.simple_inventory_mdp_cap import InventoryState\n",
    "from rl.chapter10.prediction_utils import *\n",
    "\n",
    "from rl.chapter3.simple_inventory_mdp_cap import SimpleInventoryMDPCap\n",
    "from rl.dynamic_programming import value_iteration_result\n",
    "\n",
    "from rl.distribution import Constant\n",
    "from rl.dynamic_programming import V\n",
    "import itertools\n",
    "import rl.iterate as iterate\n",
    "from rl.markov_decision_process import FiniteMarkovDecisionProcess\n",
    "from rl.policy import FiniteDeterministicPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6bdbe16-179f-449c-9337-d26e675bd69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Inventory MDP\n",
    "capacity: int = 2\n",
    "poisson_lambda: float = 1.0\n",
    "holding_cost: float = 1.0\n",
    "stockout_cost: float = 10.0\n",
    "gamma: float = 0.9\n",
    "si_mdp: SimpleInventoryMDPCap = SimpleInventoryMDPCap(\n",
    "    capacity=capacity,\n",
    "    poisson_lambda=poisson_lambda,\n",
    "    holding_cost=holding_cost,\n",
    "    stockout_cost=stockout_cost\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f03f825c-571e-47f2-bd9a-e007e6f236a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Optimal Value Function\n",
      "{NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -27.66095964467877,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -27.99189950444479,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -28.66095964467877,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -28.99189950444479,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -34.894855194671294,\n",
      " NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -29.991899504444792}\n",
      "True Optimal Policy\n",
      "For State InventoryState(on_hand=0, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=1): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=2): Do Action 0\n",
      "For State InventoryState(on_hand=1, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=1, on_order=1): Do Action 0\n",
      "For State InventoryState(on_hand=2, on_order=0): Do Action 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get Value Function and Optimal Policy via Dynamic Programming\n",
    "true_opt_vf, true_opt_policy = value_iteration_result(si_mdp, gamma=gamma)\n",
    "print(\"True Optimal Value Function\")\n",
    "pprint(true_opt_vf)\n",
    "print(\"True Optimal Policy\")\n",
    "print(true_opt_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78b095c-2fc9-42aa-b7a3-db395b10bd84",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ba89c8-89f8-4c77-a0e1-5ec693eb538b",
   "metadata": {},
   "source": [
    "## Implementing MC Control with GLIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cc923f7-6b78-45cc-abaf-e2b652abf826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definitions\n",
    "\n",
    "episode_length_tolerance: float = 1e-5\n",
    "# GLIE:\n",
    "epsilon_as_func_of_episodes: Callable[[int], float] = lambda k: k ** -1\n",
    "\n",
    "initial_learning_rate: float = 0.1\n",
    "half_life: float = 10000.0\n",
    "exponent: float = 1.0\n",
    "\n",
    "# Uniform sampling across state space:\n",
    "initial_qvf_dict = {\n",
    "    (s, a): 0. for s in si_mdp.non_terminal_states for a in si_mdp.actions(s)\n",
    "}\n",
    "\n",
    "# Function to control learning rate\n",
    "learning_rate_func: Callable[[int], float] = learning_rate_schedule(\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    half_life=half_life,\n",
    "    exponent=exponent\n",
    ")\n",
    "\n",
    "# Redefine Tabular for QValue Function Approximation\n",
    "QTabular = Tabular[Tuple[NonTerminal[S],A]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e233f581-4e3d-4d7d-bb83-0fda3b30e5c8",
   "metadata": {},
   "source": [
    "### Tabular MC Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06a95411-a547-4e8b-98a8-36a8b2321445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabular Monte Carlo Control:\n",
    "def tabular_glie_mc_control(\n",
    "    mdp: MarkovDecisionProcess[S, A],\n",
    "    states: NTStateDistribution[S],\n",
    "    approx_0: QTabular[S, A],\n",
    "    γ: float,\n",
    "    ϵ_as_func_of_episodes: Callable[[int], float],\n",
    "    episode_length_tolerance: float = 1e-6\n",
    ") -> Iterator[QTabular[S, A]]:\n",
    "    \n",
    "    q: QTabular[S, A] = approx_0\n",
    "    p: Policy[S, A] = epsilon_greedy_policy(q, mdp, 1.0) # Start off with epsilon = 1/1 = 1\n",
    "    yield q\n",
    "\n",
    "    num_episodes: int = 0\n",
    "    while True:\n",
    "        trace: Iterable[TransitionStep[S, A]] = \\\n",
    "            mdp.simulate_actions(states, p)\n",
    "        num_episodes += 1\n",
    "        for step in returns(trace, γ, episode_length_tolerance):\n",
    "            q = q.update([((step.state, step.action), step.return_)])\n",
    "        p = epsilon_greedy_policy(q, mdp, ϵ_as_func_of_episodes(num_episodes))\n",
    "        yield q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16188604-f019-46af-a6d5-63f79fd120ca",
   "metadata": {},
   "source": [
    "### Test Tabular MC Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10d4a17f-8e50-4de0-b94b-df3f526ef256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLIE MC Optimal Value Function with {num_episodes:d} episodes\n",
      "{NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -27.84635362127615,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -27.780169223503503,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -29.08649156365046,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -29.64102026775767,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -35.67519361088475,\n",
      " NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -30.829569146901967}\n",
      "GLIE MC Optimal Policy with {num_episodes:d} episodes\n",
      "For State InventoryState(on_hand=0, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=1): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=2): Do Action 0\n",
      "For State InventoryState(on_hand=1, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=1, on_order=1): Do Action 0\n",
      "For State InventoryState(on_hand=2, on_order=0): Do Action 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test Tabular MC Control:\n",
    "\n",
    "from rl.chapter11.control_utils import get_vf_and_policy_from_qvf\n",
    "\n",
    "qvfs = tabular_glie_mc_control(\n",
    "    mdp=si_mdp,\n",
    "    states=Choose(si_mdp.non_terminal_states),\n",
    "    approx_0=QTabular(\n",
    "        values_map=initial_qvf_dict,\n",
    "        count_to_weight_func=learning_rate_func\n",
    "    ),\n",
    "    γ=gamma,\n",
    "    ϵ_as_func_of_episodes=epsilon_as_func_of_episodes,\n",
    "    episode_length_tolerance=episode_length_tolerance\n",
    ")\n",
    "\n",
    "num_episodes = 10000\n",
    "final_qvf: QTabular[InventoryState, int] = \\\n",
    "    iterate.last(itertools.islice(qvfs, num_episodes))\n",
    "\n",
    "opt_vf, opt_policy = get_vf_and_policy_from_qvf(\n",
    "    mdp=si_mdp,\n",
    "    qvf=final_qvf\n",
    ")\n",
    "print(\"GLIE MC Optimal Value Function with {num_episodes:d} episodes\")\n",
    "pprint(opt_vf)\n",
    "print(\"GLIE MC Optimal Policy with {num_episodes:d} episodes\")\n",
    "print(opt_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557ded26-05b3-4956-8fbd-0c8bb3ac98ab",
   "metadata": {},
   "source": [
    "### General MC Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfe24137-8916-4a5a-8c63-a78d45d8602e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Monte Carlo Control:\n",
    "def glie_mc_control(\n",
    "    mdp: MarkovDecisionProcess[S, A],\n",
    "    states: NTStateDistribution[S],\n",
    "    approx_0: QValueFunctionApprox[S, A],\n",
    "    γ: float,\n",
    "    ϵ_as_func_of_episodes: Callable[[int], float],\n",
    "    episode_length_tolerance: float = 1e-6\n",
    ") -> Iterator[QValueFunctionApprox[S, A]]:\n",
    "   \n",
    "    q: QValueFunctionApprox[S, A] = approx_0\n",
    "    p: Policy[S, A] = epsilon_greedy_policy(q, mdp, 1.0) # Start off with epsilon = 1/1 = 1\n",
    "    yield q\n",
    "\n",
    "    num_episodes: int = 0\n",
    "    while True:\n",
    "        trace: Iterable[TransitionStep[S, A]] = \\\n",
    "            mdp.simulate_actions(states, p)\n",
    "        num_episodes += 1\n",
    "        for step in returns(trace, γ, episode_length_tolerance):\n",
    "            q = q.update([((step.state, step.action), step.return_)])\n",
    "        p = epsilon_greedy_policy(q, mdp, ϵ_as_func_of_episodes(num_episodes))\n",
    "        yield q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b1131c-580c-4b3f-817c-2f483f5c58f6",
   "metadata": {},
   "source": [
    "## Implement Asset_alloc_discrete test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "af55257a-2d33-4ff2-9c70-4ab3a644daee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.chapter7.asset_alloc_discrete import *\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "98964593-3124-439f-9378-5c11c2e9b0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps: int = 4\n",
    "μ: float = 0.13\n",
    "σ: float = 0.2\n",
    "r: float = 0.07\n",
    "a: float = 1.0\n",
    "init_wealth: float = 1.0\n",
    "init_wealth_stdev: float = 0.1\n",
    "\n",
    "excess: float = μ - r\n",
    "var: float = σ * σ\n",
    "base_alloc: float = excess / (a * var)\n",
    "\n",
    "risky_ret: Sequence[Gaussian] = [Gaussian(μ=μ, σ=σ) for _ in range(steps)]\n",
    "riskless_ret: Sequence[float] = [r for _ in range(steps)]\n",
    "utility_function: Callable[[float], float] = lambda x: - np.exp(-a * x) / a\n",
    "alloc_choices: Sequence[float] = np.linspace(\n",
    "    2 / 3 * base_alloc,\n",
    "    4 / 3 * base_alloc,\n",
    "    11\n",
    ")\n",
    "feature_funcs: Sequence[Callable[[Tuple[float, float]], float]] = \\\n",
    "    [\n",
    "        lambda _: 1.,\n",
    "        lambda w_x: w_x[0],\n",
    "        lambda w_x: w_x[1],\n",
    "        lambda w_x: w_x[1] * w_x[1]\n",
    "    ]\n",
    "dnn: DNNSpec = DNNSpec(\n",
    "    neurons=[],\n",
    "    bias=False,\n",
    "    hidden_activation=lambda x: x,\n",
    "    hidden_activation_deriv=lambda y: np.ones_like(y),\n",
    "    output_activation=lambda x: - np.sign(a) * np.exp(-x),\n",
    "    output_activation_deriv=lambda y: -y\n",
    ")\n",
    "init_wealth_distr: Gaussian = Gaussian(μ=init_wealth, σ=init_wealth_stdev)\n",
    "\n",
    "aad: AssetAllocDiscrete = AssetAllocDiscrete(\n",
    "    risky_return_distributions=risky_ret,\n",
    "    riskless_returns=riskless_ret,\n",
    "    utility_func=utility_function,\n",
    "    risky_alloc_choices=alloc_choices,\n",
    "    feature_functions=feature_funcs,\n",
    "    dnn_spec=dnn,\n",
    "    initial_wealth_distribution=init_wealth_distr\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4dce5dbe-7335-46f9-be9b-77a6c2fb9832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backward Induction: VF And Policy\n",
      "---------------------------------\n",
      "\n",
      "Time 0\n",
      "\n",
      "Opt Risky Allocation = 1.40, Opt Val = -0.212\n",
      "Weights\n",
      "[[0.2415891  1.30852079]]\n",
      "\n",
      "Time 1\n",
      "\n",
      "Opt Risky Allocation = 1.40, Opt Val = -0.246\n",
      "Weights\n",
      "[[0.17782372 1.22649341]]\n",
      "\n",
      "Time 2\n",
      "\n",
      "Opt Risky Allocation = 1.50, Opt Val = -0.283\n",
      "Weights\n",
      "[[0.1179855  1.14591944]]\n",
      "\n",
      "Time 3\n",
      "\n",
      "Opt Risky Allocation = 2.00, Opt Val = -0.323\n",
      "Weights\n",
      "[[0.05796964 1.07090809]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Optimal Value Function via ADP\n",
    "\n",
    "vf_ff: Sequence[Callable[[NonTerminal[float]], float]] = [lambda _: 1., lambda w: w.state]\n",
    "it_vf: Iterator[Tuple[DNNApprox[NonTerminal[float]], DeterministicPolicy[float, float]]] = \\\n",
    "    aad.backward_induction_vf_and_pi(vf_ff)\n",
    "\n",
    "print(\"Backward Induction: VF And Policy\")\n",
    "print(\"---------------------------------\")\n",
    "print()\n",
    "for t, (v, p) in enumerate(it_vf):\n",
    "    print(f\"Time {t:d}\")\n",
    "    print()\n",
    "    opt_alloc: float = p.action_for(init_wealth)\n",
    "    val: float = v(NonTerminal(init_wealth))\n",
    "    print(f\"Opt Risky Allocation = {opt_alloc:.2f}, Opt Val = {val:.3f}\")\n",
    "    print(\"Weights\")\n",
    "    for w in v.weights:\n",
    "        print(w.weights)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5ca0cd-86dc-4d61-a9c6-822f1e4abdc7",
   "metadata": {},
   "source": [
    "## Test General MC Control on AssetAllocDiscrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "95906069-6b54-4737-90a8-8e8bb46be21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fj/qb_tch8j6sdg7th8gw7q9w000000gn/T/ipykernel_8361/3507441780.py:33: RuntimeWarning: overflow encountered in exp\n",
      "  output_activation=lambda x: - np.sign(a) * np.exp(-x),\n"
     ]
    }
   ],
   "source": [
    "list_of_qvfs = []\n",
    "for t in range(0,4):\n",
    "\n",
    "    qvfs = glie_mc_control(\n",
    "        mdp = aad.get_mdp(t),\n",
    "        states = aad.get_states_distribution(t),\n",
    "        approx_0 = aad.get_qvf_func_approx(),\n",
    "        γ=gamma,\n",
    "        ϵ_as_func_of_episodes=epsilon_as_func_of_episodes,\n",
    "        episode_length_tolerance=episode_length_tolerance\n",
    "    )\n",
    "\n",
    "\n",
    "    final_qvf: QValueFunctionApprox[InventoryState, int] = \\\n",
    "        iterate.last(itertools.islice(qvfs, num_episodes))\n",
    "    \n",
    "    list_of_qvfs.append(final_qvf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "93d0cadb-ea0c-4db9-bc08-cb99e563b609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 0\n",
      "\n",
      "Opt Risky Allocation = 2.000, Opt Val = -0.000\n",
      "Optimal Weights below:\n",
      "array([[4.58359517, 0.02284127, 4.99079256, 6.14078238]])\n",
      "\n",
      "Time 1\n",
      "\n",
      "Opt Risky Allocation = 2.000, Opt Val = -0.000\n",
      "Optimal Weights below:\n",
      "array([[5.40012389, 0.12037042, 5.8081429 , 4.82104217]])\n",
      "\n",
      "Time 2\n",
      "\n",
      "Opt Risky Allocation = 2.000, Opt Val = -0.000\n",
      "Optimal Weights below:\n",
      "array([[4.27911185, 0.01554997, 5.11430369, 6.03359601]])\n",
      "\n",
      "Time 3\n",
      "\n",
      "Opt Risky Allocation = 1.000, Opt Val = -0.322\n",
      "Optimal Weights below:\n",
      "array([[ 1.76253743,  1.2036841 , -0.57129627, -1.26263872]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "for t, q in enumerate(list_of_qvfs):\n",
    "    print(f\"Time {t:d}\")\n",
    "    print()\n",
    "    opt_alloc: float = max(\n",
    "        ((q((NonTerminal(init_wealth), ac)), ac) for ac in alloc_choices),\n",
    "        key=itemgetter(0)\n",
    "    )[1]\n",
    "    val: float = max(q((NonTerminal(init_wealth), ac))\n",
    "                     for ac in alloc_choices)\n",
    "    print(f\"Opt Risky Allocation = {opt_alloc:.3f}, Opt Val = {val:.3f}\")\n",
    "    print(\"Optimal Weights below:\")\n",
    "    for wts in q.weights:\n",
    "        pprint(wts.weights)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19514e0a-ca4e-4d83-8fa9-a19f120eead7",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df081be2-3388-4f16-af9b-8ffba17bc0a7",
   "metadata": {},
   "source": [
    "#### Q-Learning is just SARSA, but the first action comes from the behavior policy and the second action comes from the target policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5c263f77-7ee8-4eae-99bd-5d3888c7663e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.td import epsilon_greedy_action\n",
    "\n",
    "\n",
    "# Q-Tabular\n",
    "def tabular_glie_sarsa(\n",
    "    mdp: MarkovDecisionProcess[S, A],\n",
    "    states: NTStateDistribution[S],\n",
    "    approx_0: QTabular[S, A],\n",
    "    γ: float,\n",
    "    ϵ_as_func_of_episodes: Callable[[int], float],\n",
    "    max_episode_length: int\n",
    ") -> Iterator[QTabular[S, A]]:\n",
    "    q: QTabular[S, A] = approx_0\n",
    "    yield q\n",
    "    num_episodes: int = 0\n",
    "    while True:\n",
    "        num_episodes += 1\n",
    "        ϵ: float = ϵ_as_func_of_episodes(num_episodes)\n",
    "        state: NonTerminal[S] = states.sample()\n",
    "        action: A = epsilon_greedy_action(\n",
    "            q=q,\n",
    "            nt_state=state,\n",
    "            actions=set(mdp.actions(state)),\n",
    "            ϵ=ϵ\n",
    "        )\n",
    "        steps: int = 0\n",
    "        while isinstance(state, NonTerminal) and steps < max_episode_length:\n",
    "            next_state, reward = mdp.step(state, action).sample()\n",
    "            if isinstance(next_state, NonTerminal):\n",
    "                next_action: A = epsilon_greedy_action(\n",
    "                    q=q,\n",
    "                    nt_state=next_state,\n",
    "                    actions=set(mdp.actions(next_state)),\n",
    "                    ϵ=ϵ\n",
    "                )\n",
    "                q = q.update([(\n",
    "                    (state, action),\n",
    "                    reward + γ * q((next_state, next_action))\n",
    "                )])\n",
    "                action = next_action\n",
    "            else:\n",
    "                q = q.update([((state, action), reward)])\n",
    "            yield q\n",
    "            steps += 1\n",
    "            state = next_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1036ff72-f40e-4344-9db5-a0a4bd84cc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Value Function\n",
    "\n",
    "def glie_sarsa(\n",
    "    mdp: MarkovDecisionProcess[S, A],\n",
    "    states: NTStateDistribution[S],\n",
    "    approx_0: QValueFunctionApprox[S, A],\n",
    "    γ: float,\n",
    "    ϵ_as_func_of_episodes: Callable[[int], float],\n",
    "    max_episode_length: int\n",
    ") -> Iterator[QValueFunctionApprox[S, A]]:\n",
    "    q: QValueFunctionApprox[S, A] = approx_0\n",
    "    yield q\n",
    "    num_episodes: int = 0\n",
    "    while True:\n",
    "        num_episodes += 1\n",
    "        ϵ: float = ϵ_as_func_of_episodes(num_episodes)\n",
    "        state: NonTerminal[S] = states.sample()\n",
    "        action: A = epsilon_greedy_action(\n",
    "            q=q,\n",
    "            nt_state=state,\n",
    "            actions=set(mdp.actions(state)),\n",
    "            ϵ=ϵ\n",
    "        )\n",
    "        steps: int = 0\n",
    "        while isinstance(state, NonTerminal) and steps < max_episode_length:\n",
    "            next_state, reward = mdp.step(state, action).sample()\n",
    "            if isinstance(next_state, NonTerminal):\n",
    "                next_action: A = epsilon_greedy_action(\n",
    "                    q=q,\n",
    "                    nt_state=next_state,\n",
    "                    actions=set(mdp.actions(next_state)),\n",
    "                    ϵ=ϵ\n",
    "                )\n",
    "                q = q.update([(\n",
    "                    (state, action),\n",
    "                    reward + γ * q((next_state, next_action))\n",
    "                )])\n",
    "                action = next_action\n",
    "            else:\n",
    "                q = q.update([((state, action), reward)])\n",
    "            yield q\n",
    "            steps += 1\n",
    "            state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0b81d3dc-d260-4b2b-98f9-545ccd3123c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_episode_length: int = 100\n",
    "epsilon_as_func_of_episodes: Callable[[int], float] = lambda k: k ** -0.5\n",
    "initial_learning_rate: float = 0.1\n",
    "half_life: float = 10000.0\n",
    "exponent: float = 1.0\n",
    "gamma: float = 0.9\n",
    "initial_qvf_dict: Mapping[Tuple[NonTerminal[InventoryState], int], float] = {\n",
    "    (s, a): 0. for s in si_mdp.non_terminal_states for a in si_mdp.actions(s)\n",
    "}\n",
    "learning_rate_func: Callable[[int], float] = learning_rate_schedule(\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    half_life=half_life,\n",
    "    exponent=exponent\n",
    ")\n",
    "qvfs: Iterator[QValueFunctionApprox[InventoryState, int]] = tabular_glie_sarsa(\n",
    "    mdp=si_mdp,\n",
    "    states=Choose(si_mdp.non_terminal_states),\n",
    "    approx_0=Tabular(\n",
    "        values_map=initial_qvf_dict,\n",
    "        count_to_weight_func=learning_rate_func\n",
    "    ),\n",
    "    γ=gamma,\n",
    "    ϵ_as_func_of_episodes=epsilon_as_func_of_episodes,\n",
    "    max_episode_length=max_episode_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c5a73feb-dbec-4941-a303-841c2ec03e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLIE SARSA Optimal Value Function with 1000000 updates\n",
      "{NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -27.668996755388687,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -28.95015260489432,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -28.704929438008165,\n",
      " NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -30.242889769065226,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -34.86617221756493,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -27.699963352671702}\n",
      "GLIE SARSA Optimal Policy with 1000000 updates\n",
      "For State InventoryState(on_hand=0, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=1): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=2): Do Action 0\n",
      "For State InventoryState(on_hand=1, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=1, on_order=1): Do Action 0\n",
      "For State InventoryState(on_hand=2, on_order=0): Do Action 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_updates = num_episodes * max_episode_length\n",
    "final_qvf: QValueFunctionApprox[InventoryState, int] = \\\n",
    "    iterate.last(itertools.islice(qvfs, num_updates))\n",
    "opt_vf, opt_policy = get_vf_and_policy_from_qvf(\n",
    "    mdp=si_mdp,\n",
    "    qvf=final_qvf\n",
    ")\n",
    "print(f\"GLIE SARSA Optimal Value Function with {num_updates:d} updates\")\n",
    "pprint(opt_vf)\n",
    "print(f\"GLIE SARSA Optimal Policy with {num_updates:d} updates\")\n",
    "print(opt_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6a0237-3687-40b8-a7f9-3faf64f2e865",
   "metadata": {},
   "source": [
    "### Compare to dynamic programming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bbd3a9a3-64bb-41b4-9fa3-8fe6fd553fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Optimal Value Function\n",
      "{NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -27.99189950444479,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -28.99189950444479,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -28.66095964467877,\n",
      " NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -29.991899504444792,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -34.894855194671294,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -27.66095964467877}\n",
      "True Optimal Policy\n",
      "For State InventoryState(on_hand=0, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=1): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=2): Do Action 0\n",
      "For State InventoryState(on_hand=1, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=1, on_order=1): Do Action 0\n",
      "For State InventoryState(on_hand=2, on_order=0): Do Action 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get Value Function and Optimal Policy via Dynamic Programming\n",
    "true_opt_vf, true_opt_policy = value_iteration_result(si_mdp, gamma=gamma)\n",
    "print(\"True Optimal Value Function\")\n",
    "pprint(true_opt_vf)\n",
    "print(\"True Optimal Policy\")\n",
    "print(true_opt_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ac4890-ca97-48a1-acd0-2ce56cd28917",
   "metadata": {},
   "source": [
    "## Results between Tabular SARSA and dynamic programming are very similar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e042e742-36c9-4591-b4cb-351e760ebcec",
   "metadata": {},
   "source": [
    "### Test on AssetAllocDiscrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bf8d6a46-1ceb-427d-ba45-d2c7645dadea",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_qvfs = []\n",
    "for t in range(0,4):\n",
    "\n",
    "    qvfs = glie_sarsa(\n",
    "        mdp = aad.get_mdp(t),\n",
    "        states = aad.get_states_distribution(t),\n",
    "        approx_0 = aad.get_qvf_func_approx(),\n",
    "        γ=gamma,\n",
    "        ϵ_as_func_of_episodes=epsilon_as_func_of_episodes,\n",
    "        max_episode_length=100\n",
    "    )\n",
    "\n",
    "\n",
    "    final_qvf: QValueFunctionApprox[InventoryState, int] = \\\n",
    "        iterate.last(itertools.islice(qvfs, num_episodes))\n",
    "    \n",
    "    list_of_qvfs.append(final_qvf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1f7c8afb-545e-42dc-bdd6-3d1ef8456ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 0\n",
      "\n",
      "Opt Risky Allocation = 2.000, Opt Val = -0.000\n",
      "Optimal Weights below:\n",
      "array([[4.80959858, 2.47903507, 3.82895144, 1.95879859]])\n",
      "\n",
      "Time 1\n",
      "\n",
      "Opt Risky Allocation = 2.000, Opt Val = -0.000\n",
      "Optimal Weights below:\n",
      "array([[2.95184702, 1.39973432, 2.71325493, 0.11627306]])\n",
      "\n",
      "Time 2\n",
      "\n",
      "Opt Risky Allocation = 2.000, Opt Val = -0.000\n",
      "Optimal Weights below:\n",
      "array([[2.33689038, 0.57447807, 2.19565247, 1.47547485]])\n",
      "\n",
      "Time 3\n",
      "\n",
      "Opt Risky Allocation = 1.000, Opt Val = -0.281\n",
      "Optimal Weights below:\n",
      "array([[-0.30656649,  1.27862432,  0.6007116 , -0.3043273 ]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "for t, q in enumerate(list_of_qvfs):\n",
    "    print(f\"Time {t:d}\")\n",
    "    print()\n",
    "    opt_alloc: float = max(\n",
    "        ((q((NonTerminal(init_wealth), ac)), ac) for ac in alloc_choices),\n",
    "        key=itemgetter(0)\n",
    "    )[1]\n",
    "    val: float = max(q((NonTerminal(init_wealth), ac))\n",
    "                     for ac in alloc_choices)\n",
    "    print(f\"Opt Risky Allocation = {opt_alloc:.3f}, Opt Val = {val:.3f}\")\n",
    "    print(\"Optimal Weights below:\")\n",
    "    for wts in q.weights:\n",
    "        pprint(wts.weights)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668a1695-4d65-4f21-abf3-ea2b948de71f",
   "metadata": {},
   "source": [
    "### Something still wrong for AssetAllocDiscrete implementation.... Not sure why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4be4ed-f6eb-473e-b3bc-b26c8cdd44b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
