{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d02eb1c-ee16-4884-af91-d85183855eda",
   "metadata": {},
   "source": [
    "GLIE: Greedy in the Limit with Infinite Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e5810a4-4285-4b84-8a15-6b9e4c4acf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.monte_carlo import *\n",
    "\n",
    "from rl.function_approx import Tabular\n",
    "from rl.distribution import Choose\n",
    "from rl.chapter3.simple_inventory_mdp_cap import InventoryState\n",
    "from rl.chapter10.prediction_utils import *\n",
    "\n",
    "from rl.chapter3.simple_inventory_mdp_cap import SimpleInventoryMDPCap\n",
    "from rl.dynamic_programming import value_iteration_result\n",
    "\n",
    "from rl.distribution import Constant\n",
    "from rl.dynamic_programming import V\n",
    "import itertools\n",
    "import rl.iterate as iterate\n",
    "from rl.markov_decision_process import FiniteMarkovDecisionProcess\n",
    "from rl.policy import FiniteDeterministicPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6bdbe16-179f-449c-9337-d26e675bd69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "capacity: int = 2\n",
    "poisson_lambda: float = 1.0\n",
    "holding_cost: float = 1.0\n",
    "stockout_cost: float = 10.0\n",
    "gamma: float = 0.9\n",
    "si_mdp: SimpleInventoryMDPCap = SimpleInventoryMDPCap(\n",
    "    capacity=capacity,\n",
    "    poisson_lambda=poisson_lambda,\n",
    "    holding_cost=holding_cost,\n",
    "    stockout_cost=stockout_cost\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f03f825c-571e-47f2-bd9a-e007e6f236a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Optimal Value Function\n",
      "{NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -34.894855194671294,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -27.66095964467877,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -27.99189950444479,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -28.66095964467877,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -28.99189950444479,\n",
      " NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -29.991899504444792}\n",
      "True Optimal Policy\n",
      "For State InventoryState(on_hand=0, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=1): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=2): Do Action 0\n",
      "For State InventoryState(on_hand=1, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=1, on_order=1): Do Action 0\n",
      "For State InventoryState(on_hand=2, on_order=0): Do Action 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "true_opt_vf, true_opt_policy = value_iteration_result(si_mdp, gamma=gamma)\n",
    "print(\"True Optimal Value Function\")\n",
    "pprint(true_opt_vf)\n",
    "print(\"True Optimal Policy\")\n",
    "print(true_opt_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cc923f7-6b78-45cc-abaf-e2b652abf826",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_length_tolerance: float = 1e-5\n",
    "epsilon_as_func_of_episodes: Callable[[int], float] = lambda k: k ** -0.5\n",
    "\n",
    "initial_learning_rate: float = 0.1\n",
    "half_life: float = 10000.0\n",
    "exponent: float = 1.0\n",
    "\n",
    "# Uniform sampling across state space:\n",
    "initial_qvf_dict = {\n",
    "    (s, a): 0. for s in si_mdp.non_terminal_states for a in si_mdp.actions(s)\n",
    "}\n",
    "learning_rate_func: Callable[[int], float] = learning_rate_schedule(\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    half_life=half_life,\n",
    "    exponent=exponent\n",
    ")\n",
    "qvfs = glie_mc_control(\n",
    "    mdp=si_mdp,\n",
    "    states=Choose(si_mdp.non_terminal_states),\n",
    "    approx_0=Tabular(\n",
    "        values_map=initial_qvf_dict,\n",
    "        count_to_weight_func=learning_rate_func\n",
    "    ),\n",
    "    γ=gamma,\n",
    "    ϵ_as_func_of_episodes=epsilon_as_func_of_episodes,\n",
    "    episode_length_tolerance=episode_length_tolerance\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f97081bf-211c-4069-8d90-0fc93293059d",
   "metadata": {},
   "outputs": [],
   "source": [
    "QTabular = Tabular[Tuple[NonTerminal[S],A]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d30c682d-ed4b-4e1b-b36d-b25356524f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLIE MC Optimal Value Function with {num_episodes:d} episodes\n",
      "{NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -34.96476984562754,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -27.53625227302578,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -28.586970994259534,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -28.581369243292652,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -28.825334895696155,\n",
      " NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -30.1696604747163}\n",
      "GLIE MC Optimal Policy with {num_episodes:d} episodes\n",
      "For State InventoryState(on_hand=0, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=1): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=2): Do Action 0\n",
      "For State InventoryState(on_hand=1, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=1, on_order=1): Do Action 0\n",
      "For State InventoryState(on_hand=2, on_order=0): Do Action 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tabular Monte-Carlo Control\n",
    "\n",
    "num_episodes = 10000\n",
    "final_qvf: QTabular[InventoryState, int] = \\\n",
    "    iterate.last(itertools.islice(qvfs, num_episodes))\n",
    "\n",
    "def tabular_get_vf_and_policy_from_qvf(\n",
    "    mdp: FiniteMarkovDecisionProcess[S, A],\n",
    "    qvf: QTabular[S, A]\n",
    ") -> Tuple[V[S], FiniteDeterministicPolicy[S, A]]:\n",
    "    opt_vf: V[S] = {\n",
    "        s: max(qvf((s, a)) for a in mdp.actions(s))\n",
    "        for s in mdp.non_terminal_states\n",
    "    }\n",
    "    opt_policy: FiniteDeterministicPolicy[S, A] = \\\n",
    "        FiniteDeterministicPolicy({\n",
    "            s.state: qvf.argmax((s, a) for a in mdp.actions(s))[1]\n",
    "            for s in mdp.non_terminal_states\n",
    "        })\n",
    "    return opt_vf, opt_policy\n",
    "opt_vf, opt_policy = tabular_get_vf_and_policy_from_qvf(\n",
    "    mdp=si_mdp,\n",
    "    qvf=final_qvf\n",
    ")\n",
    "print(\"GLIE MC Optimal Value Function with {num_episodes:d} episodes\")\n",
    "pprint(opt_vf)\n",
    "print(\"GLIE MC Optimal Policy with {num_episodes:d} episodes\")\n",
    "print(opt_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88a12ff9-6409-4e75-be2c-350672b472c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLIE MC Optimal Value Function with {num_episodes:d} episodes\n",
      "{NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -35.00813385042989,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -27.765313656007862,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -28.502592586157864,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -28.78768232944734,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -29.13917516743337,\n",
      " NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -30.15988517065721}\n",
      "GLIE MC Optimal Policy with {num_episodes:d} episodes\n",
      "For State InventoryState(on_hand=0, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=1): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=2): Do Action 0\n",
      "For State InventoryState(on_hand=1, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=1, on_order=1): Do Action 0\n",
      "For State InventoryState(on_hand=2, on_order=0): Do Action 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Monte-Carlo Control with Generic Function Approximation\n",
    "\n",
    "num_episodes = 10000\n",
    "final_qvf: QValueFunctionApprox[InventoryState, int] = \\\n",
    "    iterate.last(itertools.islice(qvfs, num_episodes))\n",
    "\n",
    "def get_vf_and_policy_from_qvf(\n",
    "    mdp: FiniteMarkovDecisionProcess[S, A],\n",
    "    qvf: QValueFunctionApprox[S, A]\n",
    ") -> Tuple[V[S], FiniteDeterministicPolicy[S, A]]:\n",
    "    opt_vf: V[S] = {\n",
    "        s: max(qvf((s, a)) for a in mdp.actions(s))\n",
    "        for s in mdp.non_terminal_states\n",
    "    }\n",
    "    opt_policy: FiniteDeterministicPolicy[S, A] = \\\n",
    "        FiniteDeterministicPolicy({\n",
    "            s.state: qvf.argmax((s, a) for a in mdp.actions(s))[1]\n",
    "            for s in mdp.non_terminal_states\n",
    "        })\n",
    "    return opt_vf, opt_policy\n",
    "opt_vf, opt_policy = get_vf_and_policy_from_qvf(\n",
    "    mdp=si_mdp,\n",
    "    qvf=final_qvf\n",
    ")\n",
    "print(\"GLIE MC Optimal Value Function with {num_episodes:d} episodes\")\n",
    "pprint(opt_vf)\n",
    "print(\"GLIE MC Optimal Policy with {num_episodes:d} episodes\")\n",
    "print(opt_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e702903f-133d-4b41-8551-39d14b3327e9",
   "metadata": {},
   "source": [
    "### Copy the above as the generic \"Function Approximation\" case. Replace this version with the Tabular setting. Might need to do a reclassification of Tabuluar as was done for QValueFunctionApprox[S,A]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b1131c-580c-4b3f-817c-2f483f5c58f6",
   "metadata": {},
   "source": [
    "## Implement Asset_alloc_discrete test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af55257a-2d33-4ff2-9c70-4ab3a644daee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.chapter7.asset_alloc_discrete import *\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98964593-3124-439f-9378-5c11c2e9b0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps: int = 4\n",
    "μ: float = 0.13\n",
    "σ: float = 0.2\n",
    "r: float = 0.07\n",
    "a: float = 1.0\n",
    "init_wealth: float = 1.0\n",
    "init_wealth_stdev: float = 0.1\n",
    "\n",
    "excess: float = μ - r\n",
    "var: float = σ * σ\n",
    "base_alloc: float = excess / (a * var)\n",
    "\n",
    "risky_ret: Sequence[Gaussian] = [Gaussian(μ=μ, σ=σ) for _ in range(steps)]\n",
    "riskless_ret: Sequence[float] = [r for _ in range(steps)]\n",
    "utility_function: Callable[[float], float] = lambda x: - np.exp(-a * x) / a\n",
    "alloc_choices: Sequence[float] = np.linspace(\n",
    "    2 / 3 * base_alloc,\n",
    "    4 / 3 * base_alloc,\n",
    "    11\n",
    ")\n",
    "feature_funcs: Sequence[Callable[[Tuple[float, float]], float]] = \\\n",
    "    [\n",
    "        lambda _: 1.,\n",
    "        lambda w_x: w_x[0],\n",
    "        lambda w_x: w_x[1],\n",
    "        lambda w_x: w_x[1] * w_x[1]\n",
    "    ]\n",
    "dnn: DNNSpec = DNNSpec(\n",
    "    neurons=[],\n",
    "    bias=False,\n",
    "    hidden_activation=lambda x: x,\n",
    "    hidden_activation_deriv=lambda y: np.ones_like(y),\n",
    "    output_activation=lambda x: - np.sign(a) * np.exp(-x),\n",
    "    output_activation_deriv=lambda y: -y\n",
    ")\n",
    "init_wealth_distr: Gaussian = Gaussian(μ=init_wealth, σ=init_wealth_stdev)\n",
    "\n",
    "aad: AssetAllocDiscrete = AssetAllocDiscrete(\n",
    "    risky_return_distributions=risky_ret,\n",
    "    riskless_returns=riskless_ret,\n",
    "    utility_func=utility_function,\n",
    "    risky_alloc_choices=alloc_choices,\n",
    "    feature_functions=feature_funcs,\n",
    "    dnn_spec=dnn,\n",
    "    initial_wealth_distribution=init_wealth_distr\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62ce5d35-f446-4748-b843-3981f9bd7138",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa_mdp = aad.get_mdp(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4dce5dbe-7335-46f9-be9b-77a6c2fb9832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backward Induction: VF And Policy\n",
      "---------------------------------\n",
      "\n",
      "Time 0\n",
      "\n",
      "Opt Risky Allocation = 1.70, Opt Val = -0.213\n",
      "Weights\n",
      "[[0.23191951 1.31664746]]\n",
      "\n",
      "Time 1\n",
      "\n",
      "Opt Risky Allocation = 1.00, Opt Val = -0.246\n",
      "Weights\n",
      "[[0.1729801  1.23037967]]\n",
      "\n",
      "Time 2\n",
      "\n",
      "Opt Risky Allocation = 1.70, Opt Val = -0.283\n",
      "Weights\n",
      "[[0.11618034 1.1474794 ]]\n",
      "\n",
      "Time 3\n",
      "\n",
      "Opt Risky Allocation = 1.10, Opt Val = -0.323\n",
      "Weights\n",
      "[[0.05786796 1.07137133]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Optimal Value Function via ADP\n",
    "\n",
    "vf_ff: Sequence[Callable[[NonTerminal[float]], float]] = [lambda _: 1., lambda w: w.state]\n",
    "it_vf: Iterator[Tuple[DNNApprox[NonTerminal[float]], DeterministicPolicy[float, float]]] = \\\n",
    "    aad.backward_induction_vf_and_pi(vf_ff)\n",
    "\n",
    "print(\"Backward Induction: VF And Policy\")\n",
    "print(\"---------------------------------\")\n",
    "print()\n",
    "for t, (v, p) in enumerate(it_vf):\n",
    "    print(f\"Time {t:d}\")\n",
    "    print()\n",
    "    opt_alloc: float = p.action_for(init_wealth)\n",
    "    val: float = v(NonTerminal(init_wealth))\n",
    "    print(f\"Opt Risky Allocation = {opt_alloc:.2f}, Opt Val = {val:.3f}\")\n",
    "    print(\"Weights\")\n",
    "    for w in v.weights:\n",
    "        print(w.weights)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f279ef-3495-41b5-b8d6-13bfdf3f32a2",
   "metadata": {},
   "source": [
    "## This is not the right way to do this:\n",
    "\n",
    "#### Need a finite MDP to run algorithm as currently specified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7258802d-083a-467b-a12b-816e631679bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'AssetAllocMDP' object has no attribute 'non_terminal_states'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fj/qb_tch8j6sdg7th8gw7q9w000000gn/T/ipykernel_13573/4101456419.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Uniform sampling across state space:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m initial_qvf_dict = {\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maa_mdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnon_terminal_states\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maa_mdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m }\n\u001b[1;32m      8\u001b[0m learning_rate_func: Callable[[int], float] = learning_rate_schedule(\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'AssetAllocMDP' object has no attribute 'non_terminal_states'"
     ]
    }
   ],
   "source": [
    "# Optimal Value Function via Monte-Carlo Control\n",
    "\n",
    "\n",
    "# Uniform sampling across state space:\n",
    "initial_qvf_dict = {\n",
    "    (s, a): 0. for s in aa_mdp.non_terminal_states for a in aa_mdp.actions(s)\n",
    "}\n",
    "learning_rate_func: Callable[[int], float] = learning_rate_schedule(\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    half_life=half_life,\n",
    "    exponent=exponent\n",
    ")\n",
    "qvfs = glie_mc_control(\n",
    "    mdp=aa_mdp,\n",
    "    states=Choose(aa_mdp.non_terminal_states),\n",
    "    approx_0=Tabular(\n",
    "        values_map=initial_qvf_dict,\n",
    "        count_to_weight_func=learning_rate_func\n",
    "    ),\n",
    "    γ=gamma,\n",
    "    ϵ_as_func_of_episodes=epsilon_as_func_of_episodes,\n",
    "    episode_length_tolerance=episode_length_tolerance\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f185e3-882c-4568-af38-d25e9e9edcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 10000\n",
    "final_qvf: QValueFunctionApprox[InventoryState, int] = \\\n",
    "    iterate.last(itertools.islice(qvfs, num_episodes))\n",
    "\n",
    "opt_vf, opt_policy = get_vf_and_policy_from_qvf(\n",
    "    mdp=si_mdp,\n",
    "    qvf=final_qvf\n",
    ")\n",
    "print(\"GLIE MC Optimal Value Function with {num_episodes:d} episodes\")\n",
    "pprint(opt_vf)\n",
    "print(\"GLIE MC Optimal Policy with {num_episodes:d} episodes\")\n",
    "print(opt_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb541a8-6e5a-476a-aeb9-84b8be9a0a92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
