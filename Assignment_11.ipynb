{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecaf7433-f474-4a09-bbca-bb8e280e8927",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.monte_carlo import *\n",
    "from rl.function_approx import Tabular\n",
    "from rl.distribution import Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bdfabc-13e0-45d8-8c5e-3505ae277499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction for estimating the value function\n",
    "# Traces is an iterable of trace experiences\n",
    "# A trace experience is an iterable of atomic experiences\n",
    "# An atomic experience is a TransitionStep[S], which contains\n",
    "# the current state, next state, and reward\n",
    "def tabular_mc_prediction(\n",
    "    traces: Iterable[Iterable[mp.TransitionStep[S]]],\n",
    "    tab_0: Tabular[S],\n",
    "    γ: float,\n",
    "    episode_length_tolerance: float = 1e-6\n",
    ") -> Iterator[Tabular[S]]:\n",
    "    \n",
    "    '''\n",
    "    Returns: Produces as output an Iterator of\n",
    "    Tabular[S], i.e. an updated function approximation\n",
    "    of the Value function at the end of each trace \n",
    "    experience. The updates can only be done at the end\n",
    "    of trace experiences because trace experience returns\n",
    "    are only available at the end of trace experiences.\n",
    "    '''\n",
    "    \n",
    "    episodes: Iterator[Iterator[mp.ReturnStep[S]]] = \\\n",
    "        (returns(trace, γ, episode_length_tolerance) for trace in traces)\n",
    "    f = tab_0\n",
    "    \n",
    "    num = 0\n",
    "    for trace in traces:\n",
    "        print(trace)\n",
    "        print(list(trace))\n",
    "        num += 1\n",
    "        if num > 10:\n",
    "            break\n",
    "    \n",
    "    yield f\n",
    "    \n",
    "    \n",
    "    # iterate_updates calls the method update of FunctionApprox\n",
    "    # for a single (state, return) data point. \n",
    "    for episode in episodes:\n",
    "        f = last(f.iterate_updates(\n",
    "            [(step.state, step.return_)] for step in episode\n",
    "        ))\n",
    "        yield f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5b933b-7d24-47e3-90b0-433d92a018f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def td_prediction(\n",
    "        transitions: Iterable[mp.TransitionStep[S]],\n",
    "        approx_0: Tabular[S],\n",
    "        γ: float\n",
    ") -> Iterator[Tabular[S]]:\n",
    "    '''Evaluate an MRP using TD(0) using the given sequence of\n",
    "    transitions.\n",
    "\n",
    "    Each value this function yields represents the approximated value\n",
    "    function for the MRP after an additional transition.\n",
    "\n",
    "    Arguments:\n",
    "      transitions -- a sequence of transitions from an MRP which don't\n",
    "                     have to be in order or from the same simulation\n",
    "      approx_0 -- initial approximation of value function\n",
    "      γ -- discount rate (0 < γ ≤ 1)\n",
    "\n",
    "    '''\n",
    "    def step(\n",
    "            v: Tabular[S],\n",
    "            transition: mp.TransitionStep[S]\n",
    "    ) -> tabular[S]:\n",
    "        return v.update([(\n",
    "            transition.state,\n",
    "            transition.reward + γ * extended_vf(v, transition.next_state)\n",
    "        )])\n",
    "    return iterate.accumulate(transitions, step, initial=approx_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7202b18-dedc-4f39-9b3c-30441389dfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.chapter2.simple_inventory_mrp import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4ee026-5f6d-45e2-88ca-18aaef5dd90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_capacity = 2\n",
    "user_poisson_lambda = 1.0\n",
    "user_holding_cost = 1.0\n",
    "user_stockout_cost = 10.0\n",
    "\n",
    "user_gamma = 0.9\n",
    "\n",
    "si_mrp = SimpleInventoryMRPFinite(\n",
    "    capacity=user_capacity,\n",
    "    poisson_lambda=user_poisson_lambda,\n",
    "    holding_cost=user_holding_cost,\n",
    "    stockout_cost=user_stockout_cost\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01474178-1d85-441f-891c-b5c4a9fadd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct Value Function:\n",
    "\n",
    "print(\"Value Function\")\n",
    "print(\"--------------\")\n",
    "si_mrp.display_value_function(gamma=user_gamma)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ce781b-447a-421b-a4b7-d1eeca944ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo Value Function: \n",
    "\n",
    "# 1) Create a tabular value function approximation\n",
    "# 2) Generate an iterable of trace experiences via simulation\n",
    "# 3) Run thse trace experiences through the update function of tabular_mc_prediction\n",
    "# 4) Display the value function\n",
    "\n",
    "\n",
    "# Generate Trace Experiences:\n",
    "#initial_distribution = Categorical({Inventory_State(a,b): 1\n",
    "#                                   for a in range(user_capacity)\n",
    "#                                   for b in range(user_capacity - a)})\n",
    "\n",
    "# We will just use a single trace experience, so just a single starting point\n",
    "\n",
    "initial_dist = Constant(InventoryState(0,1))\n",
    "trace_experiences = si_mrp.reward_traces(initial_dist)\n",
    "\n",
    "num = 1\n",
    "for trace in trace_experiences:\n",
    "    print(trace)\n",
    "    print(list(trace))\n",
    "    for atomic in trace:\n",
    "        print(\"printing atomic experiences\")\n",
    "        print(atomic)\n",
    "        num += 1\n",
    "        if num > 1:\n",
    "            break\n",
    "    if num > 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db4cd4f-e571-415f-b85f-498f61bf6f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I guess a single trace experience never actually ends...\n",
    "# So we should only call \n",
    "\n",
    "tv_approx = Tabular()\n",
    "value_functs = tabular_mc_prediction(trace_experiences, tv_approx, user_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5613e77-23bc-41a6-ae27-174ea70cee22",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in value_functs:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559f83d5-c68f-4121-afb0-712c59bb17e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now call the last of the generator?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3f766f-89e0-4c6d-a3b8-621799cdfe22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infinite_sequence():\n",
    "    num = 0\n",
    "    while True:\n",
    "        yield num\n",
    "        num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4c6255-9d33-4610-b119-a8183c92b35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = infinite_sequence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42575f3e-291b-4ad2-ae10-4f4bd2dd70b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4adfdb5-0c93-41ae-8e25-856c7b55d75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.chapter2.simple_inventory_mrp import *\n",
    "from rl.distribution import Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afdfb3f6-33e7-4b6b-8328-b307c4e063fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition Map\n",
      "--------------\n",
      "From State InventoryState(on_hand=0, on_order=0):\n",
      "  To State InventoryState(on_hand=0, on_order=2) with Probability 1.000\n",
      "From State InventoryState(on_hand=0, on_order=1):\n",
      "  To State InventoryState(on_hand=1, on_order=1) with Probability 0.368\n",
      "  To State InventoryState(on_hand=0, on_order=1) with Probability 0.632\n",
      "From State InventoryState(on_hand=0, on_order=2):\n",
      "  To State InventoryState(on_hand=2, on_order=0) with Probability 0.368\n",
      "  To State InventoryState(on_hand=1, on_order=0) with Probability 0.368\n",
      "  To State InventoryState(on_hand=0, on_order=0) with Probability 0.264\n",
      "From State InventoryState(on_hand=1, on_order=0):\n",
      "  To State InventoryState(on_hand=1, on_order=1) with Probability 0.368\n",
      "  To State InventoryState(on_hand=0, on_order=1) with Probability 0.632\n",
      "From State InventoryState(on_hand=1, on_order=1):\n",
      "  To State InventoryState(on_hand=2, on_order=0) with Probability 0.368\n",
      "  To State InventoryState(on_hand=1, on_order=0) with Probability 0.368\n",
      "  To State InventoryState(on_hand=0, on_order=0) with Probability 0.264\n",
      "From State InventoryState(on_hand=2, on_order=0):\n",
      "  To State InventoryState(on_hand=2, on_order=0) with Probability 0.368\n",
      "  To State InventoryState(on_hand=1, on_order=0) with Probability 0.368\n",
      "  To State InventoryState(on_hand=0, on_order=0) with Probability 0.264\n",
      "\n",
      "Transition Reward Map\n",
      "---------------------\n",
      "From State InventoryState(on_hand=0, on_order=0):\n",
      "  To [State InventoryState(on_hand=0, on_order=2) and Reward -10.000] with Probability 1.000\n",
      "From State InventoryState(on_hand=0, on_order=1):\n",
      "  To [State InventoryState(on_hand=1, on_order=1) and Reward -0.000] with Probability 0.368\n",
      "  To [State InventoryState(on_hand=0, on_order=1) and Reward -3.679] with Probability 0.632\n",
      "From State InventoryState(on_hand=0, on_order=2):\n",
      "  To [State InventoryState(on_hand=2, on_order=0) and Reward -0.000] with Probability 0.368\n",
      "  To [State InventoryState(on_hand=1, on_order=0) and Reward -0.000] with Probability 0.368\n",
      "  To [State InventoryState(on_hand=0, on_order=0) and Reward -1.036] with Probability 0.264\n",
      "From State InventoryState(on_hand=1, on_order=0):\n",
      "  To [State InventoryState(on_hand=1, on_order=1) and Reward -1.000] with Probability 0.368\n",
      "  To [State InventoryState(on_hand=0, on_order=1) and Reward -4.679] with Probability 0.632\n",
      "From State InventoryState(on_hand=1, on_order=1):\n",
      "  To [State InventoryState(on_hand=2, on_order=0) and Reward -1.000] with Probability 0.368\n",
      "  To [State InventoryState(on_hand=1, on_order=0) and Reward -1.000] with Probability 0.368\n",
      "  To [State InventoryState(on_hand=0, on_order=0) and Reward -2.036] with Probability 0.264\n",
      "From State InventoryState(on_hand=2, on_order=0):\n",
      "  To [State InventoryState(on_hand=2, on_order=0) and Reward -2.000] with Probability 0.368\n",
      "  To [State InventoryState(on_hand=1, on_order=0) and Reward -2.000] with Probability 0.368\n",
      "  To [State InventoryState(on_hand=0, on_order=0) and Reward -3.036] with Probability 0.264\n",
      "\n",
      "Stationary Distribution\n",
      "-----------------------\n",
      "{InventoryState(on_hand=0, on_order=1): 0.279,\n",
      " InventoryState(on_hand=0, on_order=0): 0.117,\n",
      " InventoryState(on_hand=1, on_order=0): 0.162,\n",
      " InventoryState(on_hand=0, on_order=2): 0.117,\n",
      " InventoryState(on_hand=1, on_order=1): 0.162,\n",
      " InventoryState(on_hand=2, on_order=0): 0.162}\n",
      "\n",
      "Reward Function\n",
      "---------------\n",
      "{NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -10.0,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -2.325,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -0.274,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -3.325,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -1.274,\n",
      " NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -2.274}\n",
      "\n",
      "Value Function\n",
      "--------------\n",
      "{NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -35.511,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -27.932,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -28.345,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -28.932,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -29.345,\n",
      " NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -30.345}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_capacity = 2\n",
    "user_poisson_lambda = 1.0\n",
    "user_holding_cost = 1.0\n",
    "user_stockout_cost = 10.0\n",
    "\n",
    "user_gamma = 0.9\n",
    "\n",
    "si_mrp = SimpleInventoryMRPFinite(\n",
    "    capacity=user_capacity,\n",
    "    poisson_lambda=user_poisson_lambda,\n",
    "    holding_cost=user_holding_cost,\n",
    "    stockout_cost=user_stockout_cost\n",
    ")\n",
    "\n",
    "from rl.markov_process import FiniteMarkovProcess\n",
    "print(\"Transition Map\")\n",
    "print(\"--------------\")\n",
    "print(FiniteMarkovProcess(\n",
    "    {s.state: Categorical({s1.state: p for s1, p in v.table().items()})\n",
    "     for s, v in si_mrp.transition_map.items()}\n",
    "))\n",
    "\n",
    "print(\"Transition Reward Map\")\n",
    "print(\"---------------------\")\n",
    "print(si_mrp)\n",
    "\n",
    "print(\"Stationary Distribution\")\n",
    "print(\"-----------------------\")\n",
    "si_mrp.display_stationary_distribution()\n",
    "print()\n",
    "\n",
    "print(\"Reward Function\")\n",
    "print(\"---------------\")\n",
    "si_mrp.display_reward_function()\n",
    "print()\n",
    "\n",
    "print(\"Value Function\")\n",
    "print(\"--------------\")\n",
    "si_mrp.display_value_function(gamma=user_gamma)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7170e37f-17b3-424a-b48a-d70f79592afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = Constant(NonTerminal(InventoryState(0,0)))\n",
    "\n",
    "a = si_mrp.simulate_reward(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "608409e5-ac24-491c-9037-beb29074bc3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object MarkovRewardProcess.simulate_reward at 0x7fd148088c10>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4bbe034e-82ba-4c00-809a-646b95928277",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 0\n",
    "for step in a:\n",
    "    print(\"printing\")\n",
    "    print(step)\n",
    "    num += 1\n",
    "    if num > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4af70900-53bf-4514-af7d-0e7ddb8fb412",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fj/qb_tch8j6sdg7th8gw7q9w000000gn/T/ipykernel_10415/943973770.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/CME241/RL-book/rl/markov_process.py\u001b[0m in \u001b[0;36msimulate_reward\u001b[0;34m(self, start_state_distribution)\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0mnext_distribution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransition_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_distribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mTransitionStep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/CME241/RL-book/rl/distribution.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0moutcomes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutcomes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;31m# TODO: Can we get rid of f or make it optional? Right now, I\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/cme241/lib/python3.8/random.py\u001b[0m in \u001b[0;36mchoices\u001b[0;34m(self, population, weights, cum_weights, k)\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mchoices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpopulation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcum_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m         \"\"\"Return a k sized list of population elements chosen with replacement.\n\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "list(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1410e9ca-e2f3-431b-95eb-fa27223bd845",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
