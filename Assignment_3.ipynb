{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a3ba470-8f08-4285-b67a-725725371a5e",
   "metadata": {},
   "source": [
    "# 1 \n",
    "The 4 Bellman Policy Equations\n",
    "\n",
    "For a deterministic policy, MDP --> MRP\n",
    "\n",
    "\n",
    "We choose action $a$ with probability 1: $a=\\pi(s)$; this action only depends on the state $s$, so I drop it from an input to $Q$ and $V$. We basically end up with the MRP equations\n",
    "\n",
    "\n",
    "\n",
    "$V^{\\pi_D}(s) = Q^{\\pi_D}(s)$ \n",
    "\n",
    "$Q^{\\pi_D}(s) = R(s) + \\gamma  \\sum_{s' \\in N} P(s,s')  V^{\\pi_D}(s')$\n",
    "\n",
    "$V^{\\pi_D}(s) = R^\\pi(s) + \\gamma \\sum_{s' \\in N} P(s,s')  V^{\\pi_D}(s')$\n",
    "\n",
    "$Q^{\\pi_D}(s) = R(s) + \\gamma  \\sum_{s' \\in N} P(s,s')  Q^{\\pi_D}(s')$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5a58ee-f1de-4b7a-88de-b4c7ccd93419",
   "metadata": {},
   "source": [
    "# 2\n",
    "\n",
    "$V_*(s) = max_{\\pi \\in \\prod} V^\\pi(s) = max_a Q_*(s,a)$\n",
    "\n",
    "$R_s(a) = a(1-a) + (1-a)(1+a) = a - a^2 + 1 - a^2 = -2a^2 + a + 1$\n",
    "\n",
    "$\\frac{\\partial R}{\\partial a} = -4a + 1$, equals $0$ when $a=0.25$\n",
    "\n",
    "$\\max R_s(a) = R_s(0.25) = 1.125$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd6a210-af6a-440e-8bc5-d8661901b0c5",
   "metadata": {},
   "source": [
    "Discount across all future states. We will choose $a=0.25$ at every step, which gives us an expected reward of 1.125 each time. Hence, we need to discount 1.125 indefinitely into the future."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6dd9f7-7031-4562-adc0-d8751a90e7bf",
   "metadata": {},
   "source": [
    "Optimal Value Function $V^*(s) = \\frac{1.125}{0.50} = 2.25$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c5ff3c-726a-4581-b230-5acdf3b8fe27",
   "metadata": {},
   "source": [
    "Optimal Deterministic Policy: $\\pi^*(s) = 0.25$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a81f94-ec2e-44e7-8cd1-76b5b6f44882",
   "metadata": {},
   "source": [
    "# 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7126d55f-238f-45d8-87aa-2392772721e0",
   "metadata": {},
   "source": [
    "State space = $\\{0, 1, 2, \\dots, n\\}$\n",
    "\n",
    "Action space = {Croak A, Croak B}\n",
    "\n",
    "$P(i, A, i-1) = \\frac{i}{n}$\n",
    "\n",
    "$P(i, A, i+1) = \\frac{n-1}{n}$\n",
    "\n",
    "$P(i, B, i') = \\frac{1}{n}$ for $i' \\neq i$ in $\\{0, 1, 2, \\dots, n\\}$\n",
    "\n",
    "$R(0) = -100$\n",
    "\n",
    "$R(n) = 10$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37de69ac-e8a8-4e8c-b1fc-c7de3fc9d7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.markov_decision_process import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6d818a8-bdbe-4aec-97af-fa71060c072d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Frog State (only attribute is an int -- the lilypad)\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class FrogState:\n",
    "    position: int"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2d4cd8-c1a1-454d-9600-d86268f3eb91",
   "metadata": {},
   "source": [
    "Let croak A = 0, croak B = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de296e5d-5cae-406e-8087-22b600d89d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 1, 1, 1, 1), (0, 1, 1, 1, 1), (1, 0, 1, 1, 1), (1, 1, 0, 1, 1), (1, 1, 1, 0, 1), (1, 1, 1, 1, 0), (0, 0, 1, 1, 1), (0, 1, 0, 1, 1), (0, 1, 1, 0, 1), (0, 1, 1, 1, 0), (1, 0, 0, 1, 1), (1, 0, 1, 0, 1), (1, 0, 1, 1, 0), (1, 1, 0, 0, 1), (1, 1, 0, 1, 0), (1, 1, 1, 0, 0), (0, 0, 0, 1, 1), (0, 0, 1, 0, 1), (0, 0, 1, 1, 0), (0, 1, 0, 0, 1), (0, 1, 0, 1, 0), (0, 1, 1, 0, 0), (1, 0, 0, 0, 1), (1, 0, 0, 1, 0), (1, 0, 1, 0, 0), (1, 1, 0, 0, 0), (0, 0, 0, 0, 1), (0, 0, 0, 1, 0), (0, 0, 1, 0, 0), (0, 1, 0, 0, 0), (1, 0, 0, 0, 0)]\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations, permutations\n",
    "from rl.policy import FiniteDeterministicPolicy\n",
    "from rl.distribution import Constant\n",
    "# Create policies\n",
    "n = 6\n",
    "\n",
    "# Create a list with all possible permutations of croaks\n",
    "# for n-2 non-terminal states where the frog makes an action.\n",
    "all_possible_permutations_of_croaks = []\n",
    "for num_A in range(0, n-1):\n",
    "    list_with_croaks = [0 for i in range(num_A)]\n",
    "    list_with_croaks += [1 for i in range(n-1 - num_A)]\n",
    "    \n",
    "    perm = list(permutations(list_with_croaks))\n",
    "    \n",
    "    # initialize a null list\n",
    "    unique_arrangements = []\n",
    "     \n",
    "    # traverse for all elements\n",
    "    for x in perm:\n",
    "        # check if exists in unique_list or not\n",
    "        if x not in unique_arrangements:\n",
    "            unique_arrangements.append(x)\n",
    "    \n",
    "    for tup in unique_arrangements:\n",
    "        all_possible_permutations_of_croaks.append(tup)\n",
    "        \n",
    "print(all_possible_permutations_of_croaks)\n",
    "    \n",
    "# List of all 2^(n-1) policies\n",
    "policies = []\n",
    "for permutation_of_croaks in all_possible_permutations_of_croaks:\n",
    "    ### What's the right policy to use???\n",
    "    ### What's the notation for a map??\n",
    "    #Check permutation_of_craosk\n",
    "    # Should just be a single list of length n\n",
    "    # NOT A LIST OF LISTS!!!!\n",
    "    # Make sure to revert markov_decision_process.py\n",
    "    fdp : FinitePolicy[FrogState, int] = \\\n",
    "    FinitePolicy(\n",
    "        {FrogState(i) : Constant(permutation_of_croaks[i-1])\n",
    "         for i in range(1, n)}\n",
    "    )\n",
    "    policies.append(fdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59c92318-9a0f-453e-a017-613d26c254e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# State to action, then from action to (next_state, reward)\n",
    "FrogMapping = Mapping[\n",
    "    FrogState,\n",
    "    Mapping[int, Categorical[Tuple[FrogState, int]]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a528d372-ac21-4110-a7a9-5985a8012584",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrogFMDP(FiniteMarkovDecisionProcess[FrogState,int]):\n",
    "    def __init__(self, num_states : int):\n",
    "        self.n = num_states\n",
    "        super().__init__(self.get_action_transition_reward_map())\n",
    "\n",
    "    def get_action_transition_reward_map(self) -> FrogMapping:\n",
    "        d: Dict[FrogState, \n",
    "                Dict[int, Categorical[Tuple[FrogState, int]]]] = {}\n",
    "\n",
    "        for i in range(1, n):\n",
    "            state: FrogState = FrogState(i)\n",
    "            \n",
    "            d1: Dict[int, Categorical[Tuple[FrogState, int]]] = {}\n",
    "            \n",
    "            # croak = 0\n",
    "            # Mapping of next_state and rewards, and probabilities\n",
    "            # of getting those next_state and reward\n",
    "            sr_probs_dict0: Dict[Tuple[FrogState, int], float] =\\\n",
    "            {(FrogState(i-1), 0) : i / self.n, \n",
    "             (FrogState(i+1), 0): (self.n-i)/self.n,\n",
    "            }\n",
    "            if i == 1:\n",
    "                # Get rid of previos key for 0 state\n",
    "                sr_probs_dict0.pop((FrogState(0), 0))\n",
    "                # Add terminal state\n",
    "                sr_probs_dict0[(FrogState(0), -100)] = i/self.n\n",
    "            elif i == n-1:\n",
    "                # Get rid of previous key for n state\n",
    "                sr_probs_dict0.pop((FrogState(n), 0))\n",
    "                # Add terminal (good) state\n",
    "                sr_probs_dict0[(FrogState(n), 10)] = (self.n-i)/self.n\n",
    "                \n",
    "            d1[0] = Categorical(sr_probs_dict0)\n",
    "            \n",
    "            \n",
    "            \n",
    "            # croak = 1\n",
    "            sr_probs_dict1: Dict[Tuple[FrogState, int], float] =\\\n",
    "            {(FrogState(i_next), 0) : 1 / self.n \n",
    "             for i_next in list(range(i)) + list(range(i+1,self.n+1))\n",
    "            }\n",
    "            \n",
    "            # Get rid of previos key for 0 state\n",
    "            sr_probs_dict1.pop((FrogState(0), 0))\n",
    "            # Add terminal state\n",
    "            sr_probs_dict1[(FrogState(0), -100)] = 1/self.n\n",
    "           \n",
    "            # Get rid of previous key for n state\n",
    "            sr_probs_dict1.pop((FrogState(self.n), 0))\n",
    "            # Add terminal (good) state\n",
    "            sr_probs_dict1[(FrogState(self.n), 10)] = 1/self.n\n",
    "                \n",
    "            d1[1] = Categorical(sr_probs_dict1)    \n",
    "                    \n",
    "            d[state] = d1\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35d83848-17ef-4b7b-b1f4-960db027fcb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-45. -45. -45. -45. -45.]\n",
      "[-57.69230769 -49.23076923 -49.23076923 -49.23076923 -49.23076923]\n",
      "[-45. -45. -45. -45. -45.]\n",
      "[-45. -45. -45. -45. -45.]\n",
      "[-45. -45. -45. -45. -45.]\n",
      "[-40.76923077 -40.76923077 -40.76923077 -40.76923077 -32.30769231]\n",
      "[-63.33333333 -56.         -52.33333333 -52.33333333 -52.33333333]\n",
      "[-57.69230769 -49.23076923 -49.23076923 -49.23076923 -49.23076923]\n",
      "[-57.69230769 -49.23076923 -49.23076923 -49.23076923 -49.23076923]\n",
      "[-54.16666667 -45.         -45.         -45.         -35.83333333]\n",
      "[-45. -45. -45. -45. -45.]\n",
      "[-45. -45. -45. -45. -45.]\n",
      "[-40.76923077 -40.76923077 -40.76923077 -40.76923077 -32.30769231]\n",
      "[-45. -45. -45. -45. -45.]\n",
      "[-40.76923077 -40.76923077 -40.76923077 -40.76923077 -32.30769231]\n",
      "[-37.66666667 -37.66666667 -37.66666667 -34.         -26.66666667]\n",
      "[-68.57142857 -62.28571429 -59.14285714 -56.         -56.        ]\n",
      "[-63.33333333 -56.         -52.33333333 -52.33333333 -52.33333333]\n",
      "[-60.10362694 -52.12435233 -48.13471503 -48.13471503 -38.44559585]\n",
      "[-57.69230769 -49.23076923 -49.23076923 -49.23076923 -49.23076923]\n",
      "[-54.16666667 -45.         -45.         -45.         -35.83333333]\n",
      "[-51.55440415 -41.86528497 -41.86528497 -37.87564767 -29.89637306]\n",
      "[-45. -45. -45. -45. -45.]\n",
      "[-40.76923077 -40.76923077 -40.76923077 -40.76923077 -32.30769231]\n",
      "[-37.66666667 -37.66666667 -37.66666667 -34.         -26.66666667]\n",
      "[-34.         -34.         -30.85714286 -27.71428571 -21.42857143]\n",
      "[-76.59574468 -71.91489362 -69.57446809 -67.23404255 -62.55319149]\n",
      "[-65.625  -58.75   -55.3125 -51.875  -41.5625]\n",
      "[-57.69230769 -49.23076923 -45.         -40.76923077 -32.30769231]\n",
      "[-48.4375 -38.125  -34.6875 -31.25   -24.375 ]\n",
      "[-27.44680851 -22.76595745 -20.42553191 -18.08510638 -13.40425532]\n"
     ]
    }
   ],
   "source": [
    "n = 6\n",
    "myFrogFMDP = FrogFMDP(n)\n",
    "\n",
    "for mypol in policies:\n",
    "    FMRP = myFrogFMDP.apply_finite_policy(policy=mypol)\n",
    "    \n",
    "    value_func = FMRP.get_value_function_vec(gamma=1)\n",
    "    print(value_func)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55b3fc3-42ed-435c-b0da-6f0ea361d7af",
   "metadata": {},
   "source": [
    "## It would appear that the last policy is the best. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a597f5ae-ae0b-430b-b055-f81186773de6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "For State FrogState(position=1):\n",
       "  Do Action 1 with Probability 1.000\n",
       "For State FrogState(position=2):\n",
       "  Do Action 0 with Probability 1.000\n",
       "For State FrogState(position=3):\n",
       "  Do Action 0 with Probability 1.000\n",
       "For State FrogState(position=4):\n",
       "  Do Action 0 with Probability 1.000\n",
       "For State FrogState(position=5):\n",
       "  Do Action 0 with Probability 1.000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policies[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ba9f70-4630-42b7-970b-a0099317be03",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
