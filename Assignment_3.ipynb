{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a3ba470-8f08-4285-b67a-725725371a5e",
   "metadata": {},
   "source": [
    "# 1 \n",
    "The 4 Bellman Policy Equations\n",
    "\n",
    "For a deterministic policy, MDP --> MRP\n",
    "\n",
    "\n",
    "We choose action $a$ with probability 1: $a=\\pi(s)$; this action only depends on the state $s$, so I drop it from an input to $Q$ and $V$. We basically end up with the MRP equations\n",
    "\n",
    "\n",
    "\n",
    "$V^{\\pi_D}(s) = Q^{\\pi_D}(s)$ \n",
    "\n",
    "$Q^{\\pi_D}(s) = R(s) + \\gamma  \\sum_{s' \\in N} P(s,s')  V^{\\pi_D}(s')$\n",
    "\n",
    "$V^{\\pi_D}(s) = R^\\pi(s) + \\gamma \\sum_{s' \\in N} P(s,s')  V^{\\pi_D}(s')$\n",
    "\n",
    "$Q^{\\pi_D}(s) = R(s) + \\gamma  \\sum_{s' \\in N} P(s,s')  Q^{\\pi_D}(s')$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5a58ee-f1de-4b7a-88de-b4c7ccd93419",
   "metadata": {},
   "source": [
    "# 2\n",
    "\n",
    "$V_*(s) = max_{\\pi \\in \\prod} V^\\pi(s) = max_a Q_*(s,a)$\n",
    "\n",
    "$R_s(a) = a(1-a) + (1-a)(1+a) = a - a^2 + 1 - a^2 = -2a^2 + a + 1$\n",
    "\n",
    "$\\frac{\\partial R}{\\partial a} = -4a + 1$, equals $0$ when $a=0.25$\n",
    "\n",
    "$\\max R_s(a) = R_s(0.25) = 1.125$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd6a210-af6a-440e-8bc5-d8661901b0c5",
   "metadata": {},
   "source": [
    "Discount across all future states. We will choose $a=0.25$ at every step, which gives us an expected reward of 1.125 each time. Hence, we need to discount 1.125 indefinitely into the future."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6dd9f7-7031-4562-adc0-d8751a90e7bf",
   "metadata": {},
   "source": [
    "Optimal Value Function $V^*(s) = \\frac{1.125}{0.50} = 2.25$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c5ff3c-726a-4581-b230-5acdf3b8fe27",
   "metadata": {},
   "source": [
    "Optimal Deterministic Policy: $\\pi^*(s) = 0.25$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a81f94-ec2e-44e7-8cd1-76b5b6f44882",
   "metadata": {},
   "source": [
    "# 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7126d55f-238f-45d8-87aa-2392772721e0",
   "metadata": {},
   "source": [
    "State space = $\\{0, 1, 2, \\dots, n\\}$\n",
    "\n",
    "Action space = {Croak A, Croak B}\n",
    "\n",
    "$P(i, A, i-1) = \\frac{i}{n}$\n",
    "\n",
    "$P(i, A, i+1) = \\frac{n-1}{n}$\n",
    "\n",
    "$P(i, B, i') = \\frac{1}{n}$ for $i' \\neq i$ in $\\{0, 1, 2, \\dots, n\\}$\n",
    "\n",
    "$R(0) = -100$\n",
    "\n",
    "$R(n) = 10$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37de69ac-e8a8-4e8c-b1fc-c7de3fc9d7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.markov_decision_process import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6d818a8-bdbe-4aec-97af-fa71060c072d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Frog State (only attribute is an int -- the lilypad)\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class FrogState:\n",
    "    position: int"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2d4cd8-c1a1-454d-9600-d86268f3eb91",
   "metadata": {},
   "source": [
    "Let croak A = 0, croak B = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de296e5d-5cae-406e-8087-22b600d89d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations, permutations\n",
    "from rl.policy import FiniteDeterministicPolicy\n",
    "from rl.distribution import Constant\n",
    "# Create policies\n",
    "n = 6\n",
    "\n",
    "# Create a list with all possible permutations of croaks\n",
    "# for n-2 non-terminal states where the frog makes an action.\n",
    "all_possible_permutations_of_croaks = []\n",
    "for num_A in range(0, n-1):\n",
    "    list_with_croaks = [0 for i in range(num_A)]\n",
    "    list_with_croaks += [1 for i in range(n-2 - num_A)]\n",
    "    \n",
    "    perm = list(permutations(list_with_croaks))\n",
    "    all_possible_permutations_of_croaks.append(perm)\n",
    "\n",
    "\n",
    "# List of all 2^(n-1) policies\n",
    "policies = []\n",
    "for permutation_of_croaks in all_possible_permutations_of_croaks:\n",
    "    ### What's the right policy to use???\n",
    "    ### What's the notation for a map??\n",
    "    \n",
    "    fdp : FinitePolicy[FrogState, int] = \\\n",
    "    FinitePolicy(\n",
    "        {FrogState(i) : Constant(permutation_of_croaks[i])\n",
    "         for i in range(1, n)}\n",
    "    )\n",
    "    \n",
    "    policies.append(fdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59c92318-9a0f-453e-a017-613d26c254e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# State to action, then from action to (next_state, reward)\n",
    "FrogMapping = Mapping[\n",
    "    FrogState,\n",
    "    Mapping[int, Categorical[Tuple[FrogState, int]]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a528d372-ac21-4110-a7a9-5985a8012584",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrogFMDP(FiniteMarkovDecisionProcess[FrogState,int]):\n",
    "    def __init__(self, num_states : int):\n",
    "        self.n = num_states\n",
    "        super().__init__(self.get_action_transition_reward_map())\n",
    "\n",
    "    def get_action_transition_reward_map(self) -> FrogMapping:\n",
    "        d: Dict[FrogState, \n",
    "                Dict[int, Categorical[Tuple[FrogState, int]]]] = {}\n",
    "\n",
    "        for i in range(1, n):\n",
    "            state: FrogState = FrogState(i)\n",
    "            \n",
    "            d1: Dict[int, Categorical[Tuple[FrogState, int]]] = {}\n",
    "            \n",
    "            # croak = 0\n",
    "            # Mapping of next_state and rewards, and probabilities\n",
    "            # of getting those next_state and reward\n",
    "            sr_probs_dict0: Dict[Tuple[FrogState, int], float] =\\\n",
    "            {(FrogState(i-1), 0) : i / self.n, \n",
    "             (FrogState(i+1), 0): (self.n-i)/self.n,\n",
    "            }\n",
    "            if i == 1:\n",
    "                # Get rid of previos key for 0 state\n",
    "                sr_probs_dict0.pop((FrogState(0), 0))\n",
    "                # Add terminal state\n",
    "                sr_probs_dict0[(FrogState(0), -100)] = i/self.n\n",
    "            elif i == n-1:\n",
    "                # Get rid of previous key for n state\n",
    "                sr_probs_dict0.pop((FrogState(n), 0))\n",
    "                # Add terminal (good) state\n",
    "                sr_probs_dict0[(FrogState(n), 10)] = (self.n-i)/self.n\n",
    "                \n",
    "            d1[0] = Categorical(sr_probs_dict0)\n",
    "            \n",
    "            \n",
    "            \n",
    "            # croak = 1\n",
    "            sr_probs_dict1: Dict[Tuple[FrogState, int], float] =\\\n",
    "            {(FrogState(i_next), 0) : 1 / self.n \n",
    "             for i_next in list(range(i)) + list(range(i+1,self.n+1))\n",
    "            }\n",
    "            \n",
    "            # Get rid of previos key for 0 state\n",
    "            sr_probs_dict1.pop((FrogState(0), 0))\n",
    "            # Add terminal state\n",
    "            sr_probs_dict1[(FrogState(0), -100)] = 1/self.n\n",
    "           \n",
    "            # Get rid of previous key for n state\n",
    "            sr_probs_dict1.pop((FrogState(self.n), 0))\n",
    "            # Add terminal (good) state\n",
    "            sr_probs_dict1[(FrogState(self.n), 10)] = 1/self.n\n",
    "                \n",
    "            d1[1] = Categorical(sr_probs_dict1)    \n",
    "                    \n",
    "            d[state] = d1\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35d83848-17ef-4b7b-b1f4-960db027fcb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constant(value=(1, 1, 1, 1))\n",
      "(1, 1, 1, 1)\n",
      "{(NonTerminal(state=FrogState(position=2)), 0): 0.8333333333333334, (Terminal(state=FrogState(position=0)), -100): 0.16666666666666666}\n",
      "{(NonTerminal(state=FrogState(position=2)), 0): 0.16666666666666666, (NonTerminal(state=FrogState(position=3)), 0): 0.16666666666666666, (NonTerminal(state=FrogState(position=4)), 0): 0.16666666666666666, (NonTerminal(state=FrogState(position=5)), 0): 0.16666666666666666, (Terminal(state=FrogState(position=0)), -100): 0.16666666666666666, (Terminal(state=FrogState(position=6)), 10): 0.16666666666666666}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "(1, 1, 1, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fj/qb_tch8j6sdg7th8gw7q9w000000gn/T/ipykernel_7339/1404282605.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmypol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpolicies\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mFMRP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyFrogFMDP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_finite_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmypol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mvalue_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFMRP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value_function_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/CME241/RL-book/rl/markov_decision_process.py\u001b[0m in \u001b[0;36mapply_finite_policy\u001b[0;34m(self, policy)\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maction_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m                     \u001b[0moutcomes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mp_action\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: (1, 1, 1, 1)"
     ]
    }
   ],
   "source": [
    "n = 6\n",
    "myFrogFMDP = FrogFMDP(n)\n",
    "\n",
    "for mypol in policies:\n",
    "    FMRP = myFrogFMDP.apply_finite_policy(policy=mypol)\n",
    "    \n",
    "    value_func = FMRP.get_value_function_vec(gamma=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e5c0a6-325a-4f81-abc9-fcf6554c0870",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d245ee0-242b-42c5-b899-e4f63ef8cc7b",
   "metadata": {},
   "source": [
    "policy.act(state) is returning the wrong actions. \n",
    "\n",
    "FinitePolicy.act() should return a finite distribution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
